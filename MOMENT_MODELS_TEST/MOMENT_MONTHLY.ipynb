{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment Applied to Price data alone (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "#!pip install numpy pandas matplotlib tqdm\n",
    "#!pip install git+https://github.com/moment-timeseries-foundation-model/moment.git\n",
    "FORECAST_HORIZON = 12\n",
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 10\n",
    "MAX_EPOCH = 2\n",
    "DATASET_PATH = './zone_data_1A_PRICE_MONTHLY.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Length: 8\n",
      "Unfrozen parameters:\n",
      "     head.linear.weight\n",
      "     head.linear.bias\n"
     ]
    }
   ],
   "source": [
    "from momentfm import MOMENTPipeline\n",
    "#Initialise the model.\n",
    "model = MOMENTPipeline.from_pretrained(\n",
    "    \"AutonLab/MOMENT-1-large\", \n",
    "    model_kwargs={\n",
    "        #can load in (1) reconstruction, (2) embedding, (3) forecasting, and (4) classification tasks\n",
    "        'task_name': 'forecasting',\n",
    "\n",
    "        # Number of steps to forecast (original is 192)\n",
    "        'forecast_horizon': FORECAST_HORIZON,\n",
    "\n",
    "        #???\n",
    "        'head_dropout': 0.1,\n",
    "\n",
    "        #???\n",
    "        'weight_decay': 0,\n",
    "        'freeze_encoder': True, # Freeze the patch embedding layer\n",
    "        'freeze_embedder': True, # Freeze the transformer encoder\n",
    "        'freeze_head': False, # The linear forecasting head must be trained\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.init()\n",
    "print(\"Patch Length:\", model.patch_len)\n",
    "# Parameters that need to be trained\n",
    "print(\"Unfrozen parameters:\")\n",
    "for name, param in model.named_parameters():    \n",
    "    if param.requires_grad:\n",
    "        print('    ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x32768 and 65536x12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# takes in tensor of shape [batchsize, n_channels, context_length]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#model return \"TimeseriesOutputs\" Object\u001b[39;00m\n\u001b[1;32m     15\u001b[0m pprint(output)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/momentfm/models/moment.py:222\u001b[0m, in \u001b[0;36mMOMENT.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TimeseriesOutputs:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/momentfm/models/moment.py:549\u001b[0m, in \u001b[0;36mMOMENT.forward\u001b[0;34m(self, x_enc, mask, input_mask, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(x_enc\u001b[38;5;241m=\u001b[39mx_enc, input_mask\u001b[38;5;241m=\u001b[39minput_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m TASKS\u001b[38;5;241m.\u001b[39mFORECASTING:\n\u001b[0;32m--> 549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m TASKS\u001b[38;5;241m.\u001b[39mCLASSIFICATION:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassify(x_enc\u001b[38;5;241m=\u001b[39mx_enc, input_mask\u001b[38;5;241m=\u001b[39minput_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/momentfm/models/moment.py:429\u001b[0m, in \u001b[0;36mMOMENT.forecast\u001b[0;34m(self, x_enc, input_mask, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m enc_out\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_channels, n_patches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39md_model))\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# [batch_size x n_channels x n_patches x d_model]\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size x n_channels x forecast_horizon]\u001b[39;00m\n\u001b[1;32m    430\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer(x\u001b[38;5;241m=\u001b[39mdec_out, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdenorm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TimeseriesOutputs(input_mask\u001b[38;5;241m=\u001b[39minput_mask, forecast\u001b[38;5;241m=\u001b[39mdec_out)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/momentfm/models/moment.py:84\u001b[0m, in \u001b[0;36mForecastingHead.forward\u001b[0;34m(self, x, input_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, input_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x32768 and 65536x12)"
     ]
    }
   ],
   "source": [
    "# Do a \"forward pass through moment\", and then examine output. MOMENT model needs 3 inputs:\n",
    "# 1. An input timeseries of 512(change later??) timesteps\n",
    "# 2. Two \"optional\" masks, both of length 512\n",
    "# \"The input mask is utilized to regulate the time steps or patches that the model should attend to. For instance, in the case of shorter time series, you may opt not to attend to padding. To implement this, you can provide an input mask with zeros in the padded locations.\"\n",
    "#\"The second mask, referred to simply as mask, denotes masked or unobserved values. We employ mask tokens to replace all patches containing any masked time step (for further details, refer to Section 3.2 in our paper). MOMENT can attend to these mask tokens during reconstruction.\"\n",
    "\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "# takes in tensor of shape [batchsize, n_channels, context_length]\n",
    "x = torch.randn(10, 1, SEQ_LEN)\n",
    "output = model(x)\n",
    "\n",
    "#model return \"TimeseriesOutputs\" Object\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "FORECAST_HORIZON = 192\n",
    "SEQ_LEN = 512\n",
    "\n",
    "class ETThDataset(Dataset):\n",
    "    def __init__(self, data, forecast_horizon=FORECAST_HORIZON, seq_len=SEQ_LEN):\n",
    "        self.data = data\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "       return max(0, len(self.data) - self.seq_len - self.forecast_horizon)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the index is within the bounds of the dataset\n",
    "        if idx + self.seq_len + self.forecast_horizon > len(self.data):\n",
    "            raise IndexError(\"Index out of bounds for dataset length.\")\n",
    "        \n",
    "        start_idx = idx\n",
    "        end_idx = start_idx + self.seq_len\n",
    "        target_end_idx = end_idx + self.forecast_horizon\n",
    "        \n",
    "        # Extract the input and target sequences\n",
    "        x = torch.tensor(self.data.iloc[start_idx:end_idx, :].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.data.iloc[end_idx:target_end_idx, -1].values, dtype=torch.float32)\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        # Assuming all data points are valid\n",
    "        input_mask = np.ones(self.seq_len, dtype=np.float32)\n",
    "        \n",
    "        return x, y, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class MyInformerDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecast_horizon: Optional[int] = FORECAST_HORIZON,\n",
    "        data_split: str = \"train\",\n",
    "        data_stride_len: int = 1,\n",
    "        task_name: str = \"forecasting\",\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        forecast_horizon : int\n",
    "            Length of the prediction sequence.\n",
    "        data_split : str\n",
    "            Split of the dataset, 'train' or 'test'.\n",
    "        data_stride_len : int\n",
    "            Stride length when generating consecutive\n",
    "            time series windows.\n",
    "        task_name : str\n",
    "            The task that the dataset is used for. One of\n",
    "            'forecasting', or  'imputation'.\n",
    "        random_seed : int\n",
    "            Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        self.seq_len = SEQ_LEN\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.full_file_path_and_name = DATASET_PATH\n",
    "        self.data_split = data_split\n",
    "        self.data_stride_len = data_stride_len\n",
    "        self.task_name = task_name\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Read data\n",
    "        self._read_data()\n",
    "\n",
    "    def _get_borders(self):\n",
    "        n_train = 12 * 30 * 24\n",
    "        n_val = 4 * 30 * 24\n",
    "        n_test = 4 * 30 * 24\n",
    "\n",
    "        train_end = n_train\n",
    "        val_end = n_train + n_val\n",
    "        test_start = val_end - self.seq_len\n",
    "        test_end = test_start + n_test + self.seq_len\n",
    "\n",
    "        train = slice(0, train_end)\n",
    "        test = slice(test_start, test_end)\n",
    "\n",
    "        return train, test\n",
    "\n",
    "    def _read_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df = pd.read_csv(self.full_file_path_and_name)\n",
    "        self.length_timeseries_original = df.shape[0]\n",
    "        self.n_channels = df.shape[1] - 1\n",
    "\n",
    "        df.drop(columns=[\"date\"], inplace=True)\n",
    "        #modify to remove copy\n",
    "        df = df.infer_objects().interpolate(method=\"cubic\")\n",
    "\n",
    "        data_splits = self._get_borders()\n",
    "\n",
    "        train_data = df[data_splits[0]]\n",
    "        self.scaler.fit(train_data.values)\n",
    "        df = self.scaler.transform(df.values)\n",
    "\n",
    "        if self.data_split == \"train\":\n",
    "            self.data = df[data_splits[0], :]\n",
    "        elif self.data_split == \"test\":\n",
    "            self.data = df[data_splits[1], :]\n",
    "\n",
    "        self.length_timeseries = self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq_start = self.data_stride_len * index\n",
    "        seq_end = seq_start + self.seq_len\n",
    "        input_mask = np.ones(self.seq_len)\n",
    "\n",
    "        if self.task_name == \"forecasting\":\n",
    "            pred_end = seq_end + self.forecast_horizon\n",
    "\n",
    "            if pred_end > self.length_timeseries:\n",
    "                pred_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.forecast_horizon\n",
    "                seq_start = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "            forecast = self.data[seq_end:pred_end, :].T\n",
    "\n",
    "            return timeseries, forecast, input_mask\n",
    "\n",
    "        elif self.task_name == \"imputation\":\n",
    "            if seq_end > self.length_timeseries:\n",
    "                seq_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "\n",
    "            return timeseries, input_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.task_name == \"imputation\":\n",
    "            return (self.length_timeseries - self.seq_len) // self.data_stride_len + 1\n",
    "        elif self.task_name == \"forecasting\":\n",
    "            return (self.length_timeseries - self.seq_len - self.forecast_horizon) // self.data_stride_len + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model FineTuning\n",
    "\"The forecasting head is randomly initialized, so it must be trained on your data.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0        date         mean         std\n",
      "0             0  2009-04-30  2150.000000  191.485422\n",
      "1             1  2009-05-31  2073.684211  299.707460\n",
      "2             2  2009-06-30  2106.350000  267.471690\n",
      "3             3  2009-07-31  2306.540897  237.066784\n",
      "4             4  2009-08-31  2140.525750  259.103054\n",
      "..          ...         ...          ...         ...\n",
      "176         176  2023-12-31  4006.749500  214.042895\n",
      "177         177  2024-01-31  4040.147273  227.444588\n",
      "178         178  2024-02-29  4002.058824  205.437883\n",
      "179         179  2024-03-31  4013.611111  202.947440\n",
      "180         180  2024-04-30  3904.163333   78.133220\n",
      "\n",
      "[181 rows x 4 columns]\n",
      "181\n",
      "144\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#train_dataset = ETThDataset(df_train, forecast_horizon=FORECAST_HORIZON)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#test_dataset = ETThDataset(df_test, forecast_horizon=FORECAST_HORIZON)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# doesn't Affects running time much! (64 default) but less makes it speed up epochs to match data loading\u001b[39;00m\n\u001b[1;32m     36\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m BATCH_SIZE\n\u001b[0;32m---> 37\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[0;31mValueError\u001b[0m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.cuda.amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from momentfm.utils.utils import control_randomness\n",
    "#from momentfm.data.informer_dataset import InformerDataset\n",
    "from momentfm.utils.forecasting_metrics import get_forecasting_metrics\n",
    "\n",
    "# Set random seeds for PyTorch, Numpy etc.\n",
    "control_randomness(seed=13) \n",
    "\n",
    "# Load data\n",
    "# Specify the path to your CSV file\n",
    "#csv_file_path = '../data/ETTh1.csv'\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "#df = df.drop(columns=[\"date\"])\n",
    "print(df)\n",
    "print(len(df))\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "print(train_size)\n",
    "df_train = df.iloc[:train_size]\n",
    "df_test = df.iloc[train_size:-(SEQ_LEN+FORECAST_HORIZON)]\n",
    "\n",
    "train_dataset = MyInformerDataset(data_split=\"train\", random_seed=13, forecast_horizon=FORECAST_HORIZON)\n",
    "test_dataset = MyInformerDataset(data_split=\"test\", random_seed=13, forecast_horizon=FORECAST_HORIZON)\n",
    "#train_dataset = ETThDataset(df_train, forecast_horizon=FORECAST_HORIZON)\n",
    "#test_dataset = ETThDataset(df_test, forecast_horizon=FORECAST_HORIZON)\n",
    "\n",
    "\n",
    "# doesn't Affects running time much! (64 default) but less makes it speed up epochs to match data loading\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cur_epoch = 0\n",
    "max_epoch = MAX_EPOCH\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the loss function to the GPU\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a OneCycleLR scheduler\n",
    "max_lr = 1e-4\n",
    "total_steps = len(train_loader) * max_epoch\n",
    "scheduler = OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.3)\n",
    "\n",
    "# Gradient clipping value\n",
    "max_norm = 5.0\n",
    "\n",
    "while cur_epoch < max_epoch:\n",
    "    losses = []\n",
    "    for timeseries, forecast, input_mask in tqdm(train_loader, total=len(train_loader)):\n",
    "        # Move the data to the GPU\n",
    "        timeseries = timeseries.float().to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        forecast = forecast.float().to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            print(timeseries.shape)\n",
    "            output = model(timeseries, input_mask)\n",
    "        \n",
    "        loss = criterion(output.forecast, forecast)\n",
    "\n",
    "        # Scales the loss for mixed precision training\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    print(f\"Epoch {cur_epoch}: Train loss: {average_loss:.3f}\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    cur_epoch += 1\n",
    "    \n",
    "    # Evaluate the model on the test split\n",
    "    trues, preds, histories, losses = [], [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for timeseries, forecast, input_mask in tqdm(test_loader):\n",
    "        # Move the data to the GPU\n",
    "            timeseries = timeseries.float().to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            forecast = forecast.float().to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                print(timeseries.shape)\n",
    "                output = model(timeseries, input_mask)\n",
    "            \n",
    "            loss = criterion(output.forecast, forecast)                \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            trues.append(forecast.detach().cpu().numpy())\n",
    "            preds.append(output.forecast.detach().cpu().numpy())\n",
    "            histories.append(timeseries.detach().cpu().numpy())\n",
    "    \n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    model.train()\n",
    "\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    histories = np.concatenate(histories, axis=0)\n",
    "    \n",
    "    metrics = get_forecasting_metrics(y=trues, y_hat=preds, reduction='mean')\n",
    "\n",
    "    print(f\"Epoch {cur_epoch}: Test MSE: {metrics.mse:.3f} | Test MAE: {metrics.mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise it!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming histories, trues, and preds are your lists containing the data\n",
    "# Extracting the first data point\n",
    "\n",
    "channel_idx = np.random.randint(0, trues.shape[1]) # There are 7 channels in this dataset\n",
    "time_index = np.random.randint(0, trues.shape[0]) \n",
    "\n",
    "history = histories[time_index, channel_idx, :] \n",
    "true = trues[time_index, channel_idx, :]\n",
    "pred = preds[time_index, channel_idx, :]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting the first time series from history\n",
    "plt.plot(range(len(history)), history, label='History (512 timesteps)', c='darkblue')\n",
    "\n",
    "# Plotting ground truth and prediction\n",
    "num_forecasts = len(true)\n",
    "\n",
    "offset = len(history)\n",
    "plt.plot(range(offset, offset + len(true)), true, label='Ground Truth (192 timesteps)', color='darkblue', linestyle='--', alpha=0.5)\n",
    "plt.plot(range(offset, offset + len(pred)), pred, label='Forecast (192 timesteps)', color='red', linestyle='--')\n",
    "\n",
    "plt.title(f\"ETTh1 (Hourly) -- (idx={time_index}, channel={channel_idx})\", fontsize=18)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
