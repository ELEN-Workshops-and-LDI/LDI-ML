{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_forecasting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemporalFusionTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_forecasting'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Open         High          Low        Close    Adj Close  \\\n",
      "0       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "1       50.555557    54.594597    50.300301    54.209209    54.209209   \n",
      "2       55.430431    56.796799    54.579578    54.754753    54.754753   \n",
      "3       55.675674    55.855858    51.836838    52.487488    52.487488   \n",
      "4       52.532532    54.054054    51.991993    53.053055    53.053055   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "4787  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4788  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4789  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4790  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4791  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "\n",
      "          Volume      CPI  Mortgage_rate  Unemp_rate        NASDAQ  \\\n",
      "0     44659096.0  189.800         5.7540         5.4   1884.730952   \n",
      "1     22834343.0  189.800         5.7540         5.4   1884.730952   \n",
      "2     18256126.0  189.800         5.7540         5.4   1884.730952   \n",
      "3     15247337.0  189.800         5.7540         5.4   1884.730952   \n",
      "4      9188602.0  189.800         5.7540         5.4   1884.730952   \n",
      "...          ...      ...            ...         ...           ...   \n",
      "4787   1317900.0  268.599         2.9625         5.8  13517.682500   \n",
      "4788   1317900.0  273.092         2.8425         5.2  14856.517727   \n",
      "4789   1317900.0  281.933         3.4450         4.0  14531.377500   \n",
      "4790   1317900.0  288.663         4.9825         3.6  13394.163000   \n",
      "4791   1317900.0  291.474         5.2300         3.6  11829.729524   \n",
      "\n",
      "      disposable_income  Personal_consumption_expenditure  personal_savings  \n",
      "0               10701.4                            8341.5               4.4  \n",
      "1               10701.4                            8341.5               4.4  \n",
      "2               10701.4                            8341.5               4.4  \n",
      "3               10701.4                            8341.5               4.4  \n",
      "4               10701.4                            8341.5               4.4  \n",
      "...                 ...                               ...               ...  \n",
      "4787            15669.5                           15624.4              10.4  \n",
      "4788            15720.0                           15991.1               9.8  \n",
      "4789            15163.5                           16543.3               5.8  \n",
      "4790            15154.4                           16911.2               5.2  \n",
      "4791            15144.8                           16954.5               5.4  \n",
      "\n",
      "[4792 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load SP500 and US macroeconomic data from CSV files\n",
    "data = pd.read_csv('./data/SP500_data/GOOGL.csv', parse_dates=['Date'])\n",
    "macro_data = pd.read_csv('./data/SP500_data/US_macroeconomics.csv', parse_dates=['date'])\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)  # Set the date column as the index if not already set\n",
    "\n",
    "# Merge datasets on the date field\n",
    "merged_data = pd.merge(data, macro_data, left_on='Date', right_on='date', how='outer')\n",
    "\n",
    "# Handle missing values: forward fill then backward fill to cover all gaps\n",
    "merged_data.interpolate(method='ffill', inplace=True)\n",
    "merged_data.interpolate(method='bfill', inplace=True)\n",
    "\n",
    "# Drop the extra date column to avoid duplication\n",
    "merged_data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "print(merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (800, 13)\n",
      "Test data shape: (200, 13)\n",
      "           Open        High         Low       Close   Adj Close      Volume  \\\n",
      "0     50.050049   52.082081   48.028027   50.220219   50.220219  44659096.0   \n",
      "1     50.555557   54.594597   50.300301   54.209209   54.209209  22834343.0   \n",
      "2     55.430431   56.796799   54.579578   54.754753   54.754753  18256126.0   \n",
      "3     55.675674   55.855858   51.836838   52.487488   52.487488  15247337.0   \n",
      "4     52.532532   54.054054   51.991993   53.053055   53.053055   9188602.0   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "795  309.554565  313.273285  306.301300  308.308319  308.308319  12038549.0   \n",
      "796  315.540527  317.317322  311.106110  317.057068  317.057068  12048939.0   \n",
      "797  318.023010  321.006012  314.564575  320.130127  320.130127  24553822.0   \n",
      "798  327.607605  329.574585  321.936951  322.677673  322.677673  31546422.0   \n",
      "799  319.654663  327.827820  318.458466  325.700714  325.700714  13315471.0   \n",
      "\n",
      "        CPI  Mortgage_rate  Unemp_rate       NASDAQ  disposable_income  \\\n",
      "0    189.80          5.754         5.4  1884.730952            10701.4   \n",
      "1    189.80          5.754         5.4  1884.730952            10701.4   \n",
      "2    189.80          5.754         5.4  1884.730952            10701.4   \n",
      "3    189.80          5.754         5.4  1884.730952            10701.4   \n",
      "4    189.80          5.754         5.4  1884.730952            10701.4   \n",
      "..      ...            ...         ...          ...                ...   \n",
      "795  209.19          6.375         4.7  2780.415652            11523.6   \n",
      "796  209.19          6.375         4.7  2780.415652            11523.6   \n",
      "797  209.19          6.375         4.7  2780.415652            11523.6   \n",
      "798  209.19          6.375         4.7  2780.415652            11523.6   \n",
      "799  209.19          6.375         4.7  2780.415652            11523.6   \n",
      "\n",
      "     Personal_consumption_expenditure  personal_savings  \n",
      "0                              8341.5               4.4  \n",
      "1                              8341.5               4.4  \n",
      "2                              8341.5               4.4  \n",
      "3                              8341.5               4.4  \n",
      "4                              8341.5               4.4  \n",
      "..                                ...               ...  \n",
      "795                            9882.7               3.0  \n",
      "796                            9882.7               3.0  \n",
      "797                            9882.7               3.0  \n",
      "798                            9882.7               3.0  \n",
      "799                            9882.7               3.0  \n",
      "\n",
      "[800 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Limit the dataset to the first 1000 rows\n",
    "limited_data = merged_data.head(1000)\n",
    "\n",
    "# Split the limited data into train and test sets\n",
    "train_data = limited_data[:int(0.8*len(limited_data))]\n",
    "test_data = limited_data[int(0.8*len(limited_data)):]\n",
    "\n",
    "# Display the shapes of the train and test data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 73\u001b[0m\n\u001b[1;32m     69\u001b[0m tft_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Assuming train_data, val_data, and test_data are prepared and available\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     76\u001b[0m predictions \u001b[38;5;241m=\u001b[39m tft_model\u001b[38;5;241m.\u001b[39mpredict(test_data)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optree/ops.py:594\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    592\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    593\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the TFT model\n",
    "class StockPriceTFT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tft = TemporalFusionTransformer(\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            lstm_layers=config[\"lstm_layers\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            output_size=config[\"output_size\"],\n",
    "            loss=config[\"loss\"],\n",
    "            attention_head_size=config[\"attention_head_size\"],\n",
    "            max_encoder_length=config[\"max_encoder_length\"],\n",
    "            static_categoricals=config[\"static_categoricals\"],\n",
    "            static_reals=config[\"static_reals\"],\n",
    "            time_varying_categoricals_encoder=config[\"time_varying_categoricals_encoder\"],\n",
    "            time_varying_categoricals_decoder=config[\"time_varying_categoricals_decoder\"],\n",
    "            categorical_groups=config[\"categorical_groups\"],\n",
    "            time_varying_reals_encoder=config[\"time_varying_reals_encoder\"],\n",
    "            time_varying_reals_decoder=config[\"time_varying_reals_decoder\"],\n",
    "            x_reals=config[\"x_reals\"],\n",
    "            x_categoricals=config[\"x_categoricals\"],\n",
    "            hidden_continuous_size=config[\"hidden_continuous_size\"],\n",
    "            hidden_continuous_sizes=config[\"hidden_continuous_sizes\"],\n",
    "            embedding_sizes=config[\"embedding_sizes\"],\n",
    "            embedding_paddings=config[\"embedding_paddings\"],\n",
    "            embedding_labels=config[\"embedding_labels\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            log_interval=config[\"log_interval\"],\n",
    "            log_val_interval=config[\"log_val_interval\"],\n",
    "            log_gradient_flow=config[\"log_gradient_flow\"],\n",
    "            reduce_on_plateau_patience=config[\"reduce_on_plateau_patience\"],\n",
    "            monotone_constaints=config[\"monotone_constaints\"],\n",
    "            share_single_variable_networks=config[\"share_single_variable_networks\"],\n",
    "            causal_attention=config[\"causal_attention\"],\n",
    "            logging_metrics=config[\"logging_metrics\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tft(x)\n",
    "\n",
    "# Configure the TFT model\n",
    "config = {\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 1,\n",
    "    \"loss\": nn.MSELoss(),\n",
    "    \"attention_head_size\": 4,\n",
    "    \"max_encoder_length\": 30,\n",
    "    \"static_categoricals\": [],\n",
    "    \"static_reals\": [],\n",
    "    \"time_varying_categoricals_encoder\": [],\n",
    "    \"time_varying_categoricals_decoder\": [],\n",
    "    \"categorical_groups\": {},\n",
    "    \"time_varying_reals_encoder\": [\"price\"],\n",
    "    \"time_varying_reals_decoder\": [\"price\"],\n",
    "    \"x_reals\": [\"price\"],\n",
    "    \"x_categoricals\": [],\n",
    "    \"hidden_continuous_size\": 16,\n",
    "    \"hidden_continuous_sizes\": {},\n",
    "    \"embedding_sizes\": {},\n",
    "    \"embedding_paddings\": [],\n",
    "    \"embedding_labels\": {},\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"log_interval\": 10,\n",
    "    \"log_val_interval\": 10,\n",
    "    \"log_gradient_flow\": False,\n",
    "    \"reduce_on_plateau_patience\": 10,\n",
    "    \"monotone_constaints\": {},\n",
    "    \"share_single_variable_networks\": False,\n",
    "    \"causal_attention\": True,\n",
    "    \"logging_metrics\": None\n",
    "}\n",
    "\n",
    "# Create the TFT model\n",
    "model = StockPriceTFT(config)\n",
    "\n",
    "# Prepare your data\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = config[\"loss\"](output, batch[\"target\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch in test_data:\n",
    "            output = model(batch)\n",
    "            val_loss += config[\"loss\"](output, batch[\"target\"]).item()\n",
    "        val_loss /= len(test_data)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
