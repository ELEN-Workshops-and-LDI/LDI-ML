{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Open         High          Low        Close    Adj Close  \\\n",
      "0       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "1       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "2       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "3       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "4       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "4787  2722.030029  2821.000000  2722.030029  2797.360107  2797.360107   \n",
      "4788  2774.050049  2791.770020  2756.699951  2765.510010  2765.510010   \n",
      "4789  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4790  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4791  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "\n",
      "          Volume      CPI  Mortgage_rate  Unemp_rate        NASDAQ  \\\n",
      "0     44659096.0   85.600        14.2050         7.5    200.685556   \n",
      "1     44659096.0   86.400        14.7900         7.2    198.398636   \n",
      "2     44659096.0   87.200        14.9040         7.5    198.817619   \n",
      "3     44659096.0   88.000        15.1325         7.4    194.852105   \n",
      "4     44659096.0   88.600        15.4000         7.4    203.593182   \n",
      "...          ...      ...            ...         ...           ...   \n",
      "4787   1774800.0  287.708         4.1720         3.6  13623.263043   \n",
      "4788   1257700.0  287.708         4.1720         3.6  13623.263043   \n",
      "4789   1317900.0  287.708         4.1720         3.6  13623.263043   \n",
      "4790   1317900.0  288.663         4.9825         3.6  13394.163000   \n",
      "4791   1317900.0  291.474         5.2300         3.6  11829.729524   \n",
      "\n",
      "      disposable_income  Personal_consumption_expenditure  personal_savings  \n",
      "0                4976.5                            1826.8              11.6  \n",
      "1                4999.8                            1851.7              11.4  \n",
      "2                4980.4                            1870.0              10.9  \n",
      "3                4965.0                            1884.2              10.8  \n",
      "4                4979.0                            1902.9              10.8  \n",
      "...                 ...                               ...               ...  \n",
      "4787            15119.6                           16831.2               5.3  \n",
      "4788            15119.6                           16831.2               5.3  \n",
      "4789            15119.6                           16831.2               5.3  \n",
      "4790            15154.4                           16911.2               5.2  \n",
      "4791            15144.8                           16954.5               5.4  \n",
      "\n",
      "[4792 rows x 13 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/x_bnq71n58lbk4_znq_twz6h0000gn/T/ipykernel_3878/1542437413.py:12: FutureWarning: DataFrame.interpolate with method=ffill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.interpolate(method='ffill', inplace=True)\n",
      "/var/folders/d9/x_bnq71n58lbk4_znq_twz6h0000gn/T/ipykernel_3878/1542437413.py:13: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.interpolate(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load SP500 and US macroeconomic data from CSV files\n",
    "data = pd.read_csv('./data/SP500_data/GOOGL.csv', parse_dates=['Date'])\n",
    "macro_data = pd.read_csv('./data/SP500_data/US_macroeconomics.csv', parse_dates=['date'])\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)  # Set the date column as the index if not already set\n",
    "\n",
    "# Merge datasets on the date field\n",
    "merged_data = pd.merge(data, macro_data, left_on='Date', right_on='date', how='outer')\n",
    "\n",
    "# Handle missing values: forward fill then backward fill to cover all gaps\n",
    "merged_data.interpolate(method='ffill', inplace=True)\n",
    "merged_data.interpolate(method='bfill', inplace=True)\n",
    "\n",
    "# Drop the extra date column to avoid duplication\n",
    "merged_data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "print(merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (800, 13)\n",
      "Test data shape: (200, 13)\n"
     ]
    }
   ],
   "source": [
    "# Limit the dataset to the first 1000 rows\n",
    "limited_data = merged_data.head(1000)\n",
    "\n",
    "# Split the limited data into train and test sets\n",
    "train_data = limited_data[:int(0.8*len(limited_data))]\n",
    "test_data = limited_data[int(0.8*len(limited_data)):]\n",
    "\n",
    "# Display the shapes of the train and test data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.StockPriceDataset object at 0x2a158e340>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define input columns and specify which are categorical and which are continuous\n",
    "input_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"CPI\", \"Mortgage_rate\", \"Unemp_rate\", \"NASDAQ\", \"disposable_income\", \"Personal_consumption_expenditure\", \"personal_savings\"]\n",
    "continuous_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]  # Example continuous columns\n",
    "categorical_columns = [\"CPI\", \"Mortgage_rate\", \"Unemp_rate\", \"NASDAQ\", \"disposable_income\", \"Personal_consumption_expenditure\", \"personal_savings\"]  # Example categorical columns\n",
    "target_column = \"Close\"  # Assuming you want to predict the closing price\n",
    "\n",
    "class StockPriceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_cat = self.data.loc[idx, categorical_columns].values\n",
    "        x_cont = self.data.loc[idx, continuous_columns].values\n",
    "        y = self.data.loc[idx, target_column]\n",
    "        # Assuming all sequences are of the same length, which might need adjustment based on your actual data\n",
    "        encoder_length = len(x_cat)  # or some other logic to determine the actual length\n",
    "        decoder_length = len(x_cat)  # similarly, adjust as necessary\n",
    "\n",
    "        return {\n",
    "            \"encoder_cat\": torch.tensor(x_cat, dtype=torch.float32).unsqueeze(0),\n",
    "            \"encoder_cont\": torch.tensor(x_cont, dtype=torch.float32).unsqueeze(0),\n",
    "            \"decoder_cat\": torch.tensor(x_cat, dtype=torch.float32).unsqueeze(0),\n",
    "            \"decoder_cont\": torch.tensor(x_cont, dtype=torch.float32).unsqueeze(0),\n",
    "            \"encoder_lengths\": torch.tensor([encoder_length], dtype=torch.long),\n",
    "            \"decoder_lengths\": torch.tensor([decoder_length], dtype=torch.long),\n",
    "            \"target\": torch.tensor(y, dtype=torch.float32).unsqueeze(0),\n",
    "        }\n",
    "\n",
    "train_dataset = StockPriceDataset(train_data)\n",
    "test_dataset = StockPriceDataset(test_data)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LightningModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MeanSquaredError\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the TFT model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStockPriceTFT\u001b[39;00m(\u001b[43mLightningModule\u001b[49m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LightningModule' is not defined"
     ]
    }
   ],
   "source": [
    "from torchmetrics import MeanSquaredError\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "\n",
    "# Define the TFT model\n",
    "class StockPriceTFT(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['loss', 'logging_metrics'])\n",
    "        self.tft = TemporalFusionTransformer(\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            lstm_layers=config[\"lstm_layers\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            output_size=config[\"output_size\"],\n",
    "            loss=config[\"loss\"],\n",
    "            attention_head_size=config[\"attention_head_size\"],\n",
    "            max_encoder_length=config[\"max_encoder_length\"],\n",
    "            static_categoricals=config[\"static_categoricals\"],\n",
    "            static_reals=config[\"static_reals\"],\n",
    "            time_varying_categoricals_encoder=config[\"time_varying_categoricals_encoder\"],\n",
    "            time_varying_categoricals_decoder=config[\"time_varying_categoricals_decoder\"],\n",
    "            categorical_groups=config[\"categorical_groups\"],\n",
    "            time_varying_reals_encoder=config[\"time_varying_reals_encoder\"],\n",
    "            time_varying_reals_decoder=config[\"time_varying_reals_decoder\"],\n",
    "            x_reals=config[\"x_reals\"],\n",
    "            x_categoricals=config[\"x_categoricals\"],\n",
    "            hidden_continuous_size=config[\"hidden_continuous_size\"],\n",
    "            hidden_continuous_sizes=config[\"hidden_continuous_sizes\"],\n",
    "            embedding_sizes=config[\"embedding_sizes\"],\n",
    "            embedding_paddings=config[\"embedding_paddings\"],\n",
    "            embedding_labels=config[\"embedding_labels\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            log_interval=config[\"log_interval\"],\n",
    "            log_val_interval=config[\"log_val_interval\"],\n",
    "            log_gradient_flow=config[\"log_gradient_flow\"],\n",
    "            reduce_on_plateau_patience=config[\"reduce_on_plateau_patience\"],\n",
    "            monotone_constaints=config[\"monotone_constaints\"],\n",
    "            share_single_variable_networks=config[\"share_single_variable_networks\"],\n",
    "            causal_attention=config[\"causal_attention\"],\n",
    "            logging_metrics=config[\"logging_metrics\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Example debug statements before a problematic layer\n",
    "        print(\"Before layer X, shape of input:\", x[\"encoder_cont\"].shape)\n",
    "        # layer operations\n",
    "        output = self.layer_x(x[\"encoder_cont\"])\n",
    "        print(\"After layer X, output shape:\", output.shape)\n",
    "        # more operations\n",
    "        return output\n",
    "# Configure the TFT model\n",
    "config = {\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 1,\n",
    "    \"loss\": MeanSquaredError(),\n",
    "    \"attention_head_size\": 4,\n",
    "    \"max_encoder_length\": 30,\n",
    "    \"static_categoricals\": [],\n",
    "    \"static_reals\": [],\n",
    "    \"time_varying_categoricals_encoder\": [],\n",
    "    \"time_varying_categoricals_decoder\": [],\n",
    "    \"categorical_groups\": {},\n",
    "    \"time_varying_reals_encoder\": [\"price\"],\n",
    "    \"time_varying_reals_decoder\": [\"price\"],\n",
    "    \"x_reals\": [\"price\"],\n",
    "    \"x_categoricals\": [],\n",
    "    \"hidden_continuous_size\": 16,\n",
    "    \"hidden_continuous_sizes\": {},\n",
    "    \"embedding_sizes\": {},\n",
    "    \"embedding_paddings\": [],\n",
    "    \"embedding_labels\": {},\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"log_interval\": 10,\n",
    "    \"log_val_interval\": 10,\n",
    "    \"log_gradient_flow\": False,\n",
    "    \"reduce_on_plateau_patience\": 10,\n",
    "    \"monotone_constaints\": {},\n",
    "    \"share_single_variable_networks\": False,\n",
    "    \"causal_attention\": True,\n",
    "    \"logging_metrics\": None\n",
    "}\n",
    "\n",
    "# Create the TFT model\n",
    "model = StockPriceTFT(config)\n",
    "\n",
    "# Prepare your data\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        print(\"Batch keys:\", batch.keys())\n",
    "        print(\"Shape of encoder_cat:\", batch[\"encoder_cat\"].shape)\n",
    "        print(\"Shape of encoder_cont:\", batch[\"encoder_cont\"].shape)\n",
    "        print(\"Shape of decoder_cat:\", batch[\"decoder_cat\"].shape)\n",
    "        print(\"Shape of decoder_cont:\", batch[\"decoder_cont\"].shape)\n",
    "        print(\"Shape of target:\", batch[\"target\"].shape)\n",
    "    \n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model({\n",
    "            \"encoder_cat\": batch[\"encoder_cat\"].squeeze(1),\n",
    "            \"encoder_cont\": batch[\"encoder_cont\"].squeeze(1),\n",
    "            \"decoder_cat\": batch[\"decoder_cat\"].squeeze(1),\n",
    "            \"decoder_cont\": batch[\"decoder_cont\"].squeeze(1),\n",
    "            \"encoder_lengths\": batch[\"encoder_lengths\"].squeeze(1),\n",
    "            \"decoder_lengths\": batch[\"decoder_lengths\"].squeeze(1)\n",
    "        })\n",
    "        loss = config[\"loss\"](output, batch[\"target\"].squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch in test_loader:\n",
    "            output = model(batch[\"x\"])\n",
    "            val_loss += config[\"loss\"](output, batch[\"target\"]).item()\n",
    "        val_loss /= len(test_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
