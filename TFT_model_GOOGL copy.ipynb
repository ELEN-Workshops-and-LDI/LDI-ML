{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from torchmetrics import MeanSquaredError\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchmetrics import MeanSquaredError\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl # Instead of import pytorch_lightning as pl\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from pytorch_forecasting import DeepAR, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'train_data' and 'test_data' are already defined and preprocessed as per your previous code\n",
    "\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Open         High          Low        Close    Adj Close  \\\n",
      "0       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "1       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "2       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "3       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "4       50.050049    52.082081    48.028027    50.220219    50.220219   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "4787  2722.030029  2821.000000  2722.030029  2797.360107  2797.360107   \n",
      "4788  2774.050049  2791.770020  2756.699951  2765.510010  2765.510010   \n",
      "4789  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4790  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "4791  2784.000000  2832.379883  2755.010010  2831.439941  2831.439941   \n",
      "\n",
      "          Volume      CPI  Mortgage_rate  Unemp_rate        NASDAQ  \\\n",
      "0     44659096.0   85.600        14.2050         7.5    200.685556   \n",
      "1     44659096.0   86.400        14.7900         7.2    198.398636   \n",
      "2     44659096.0   87.200        14.9040         7.5    198.817619   \n",
      "3     44659096.0   88.000        15.1325         7.4    194.852105   \n",
      "4     44659096.0   88.600        15.4000         7.4    203.593182   \n",
      "...          ...      ...            ...         ...           ...   \n",
      "4787   1774800.0  287.708         4.1720         3.6  13623.263043   \n",
      "4788   1257700.0  287.708         4.1720         3.6  13623.263043   \n",
      "4789   1317900.0  287.708         4.1720         3.6  13623.263043   \n",
      "4790   1317900.0  288.663         4.9825         3.6  13394.163000   \n",
      "4791   1317900.0  291.474         5.2300         3.6  11829.729524   \n",
      "\n",
      "      disposable_income  Personal_consumption_expenditure  personal_savings  \n",
      "0                4976.5                            1826.8              11.6  \n",
      "1                4999.8                            1851.7              11.4  \n",
      "2                4980.4                            1870.0              10.9  \n",
      "3                4965.0                            1884.2              10.8  \n",
      "4                4979.0                            1902.9              10.8  \n",
      "...                 ...                               ...               ...  \n",
      "4787            15119.6                           16831.2               5.3  \n",
      "4788            15119.6                           16831.2               5.3  \n",
      "4789            15119.6                           16831.2               5.3  \n",
      "4790            15154.4                           16911.2               5.2  \n",
      "4791            15144.8                           16954.5               5.4  \n",
      "\n",
      "[4792 rows x 13 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/x_bnq71n58lbk4_znq_twz6h0000gn/T/ipykernel_11364/1542437413.py:12: FutureWarning: DataFrame.interpolate with method=ffill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.interpolate(method='ffill', inplace=True)\n",
      "/var/folders/d9/x_bnq71n58lbk4_znq_twz6h0000gn/T/ipykernel_11364/1542437413.py:13: FutureWarning: DataFrame.interpolate with method=bfill is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.interpolate(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load SP500 and US macroeconomic data from CSV files\n",
    "data = pd.read_csv('./data/SP500_data/GOOGL.csv', parse_dates=['Date'])\n",
    "macro_data = pd.read_csv('./data/SP500_data/US_macroeconomics.csv', parse_dates=['date'])\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)  # Set the date column as the index if not already set\n",
    "\n",
    "# Merge datasets on the date field\n",
    "merged_data = pd.merge(data, macro_data, left_on='Date', right_on='date', how='outer')\n",
    "\n",
    "# Handle missing values: forward fill then backward fill to cover all gaps\n",
    "merged_data.interpolate(method='ffill', inplace=True)\n",
    "merged_data.interpolate(method='bfill', inplace=True)\n",
    "\n",
    "# Drop the extra date column to avoid duplication\n",
    "merged_data.drop(columns=['date'], inplace=True)\n",
    "\n",
    "print(merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (80, 13)\n",
      "Test data shape: (20, 13)\n",
      "         Open       High        Low      Close  Adj Close      Volume         CPI  Mortgage_rate  Unemp_rate      NASDAQ  disposable_income  Personal_consumption_expenditure  personal_savings  time_idx  group_id\n",
      "0   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   85.599998      14.205000         7.5  200.685562        4976.500000                       1826.800049              11.6         0         1\n",
      "1   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   86.400002      14.790000         7.2  198.398636        4999.799805                       1851.699951              11.4         1         1\n",
      "2   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   87.199997      14.904000         7.5  198.817612        4980.399902                       1870.000000              10.9         2         1\n",
      "3   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   88.000000      15.132500         7.4  194.852112        4965.000000                       1884.199951              10.8         3         1\n",
      "4   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   88.599998      15.400000         7.4  203.593185        4979.000000                       1902.900024              10.8         4         1\n",
      "5   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   89.099998      15.580000         7.2  215.119995        4965.100098                       1904.400024              10.9         5         1\n",
      "6   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   89.699997      16.402000         7.5  216.543503        4974.799805                       1913.800049              11.0         6         1\n",
      "7   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   90.500000      16.695000         7.5  220.040451        5001.899902                       1934.500000              10.8         7         1\n",
      "8   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   91.500000      16.832001         7.2  209.720901        5080.799805                       1942.099976              12.3         8         1\n",
      "9   50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   92.199997      17.285000         7.4  206.626190        5095.899902                       1966.599976              12.0         9         1\n",
      "10  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   93.099998      18.160000         7.6  185.372375        5087.200195                       1965.500000              12.4        10         1\n",
      "11  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   93.400002      18.454000         7.9  190.201370        5093.799805                       1963.900024              13.0        11         1\n",
      "12  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   93.800003      17.825001         8.3  199.229004        5096.899902                       1970.599976              13.2        12         1\n",
      "13  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   94.099998      16.945999         8.5  197.149551        5090.200195                       1988.800049              12.5        13         1\n",
      "14  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   94.400002      17.485001         8.6  187.678497        5096.899902                       1997.099976              12.7        14         1\n",
      "15  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   94.699997      17.597500         8.9  182.628418        5110.799805                       2021.199951              12.1        15         1\n",
      "16  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   94.699997      17.160000         9.0  173.283920        5110.000000                       2024.099976              12.2        16         1\n",
      "17  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   95.000000      16.892000         9.3  181.844284        5160.600098                       2026.300049              12.9        17         1\n",
      "18  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   95.900002      16.677500         9.4  184.606995        5139.000000                       2044.500000              12.3        18         1\n",
      "19  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   97.000000      16.697500         9.6  172.074997        5110.700195                       2048.100098              12.3        19         1\n",
      "20  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   97.500000      16.816000         9.8  169.625244        5157.000000                       2072.199951              12.5        20         1\n",
      "21  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   97.699997      16.270000         9.8  167.382721        5164.700195                       2080.100098              12.6        21         1\n",
      "22  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   97.699997      15.430000        10.1  185.092850        5162.399902                       2104.600098              11.8        22         1\n",
      "23  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   98.099998      14.608000        10.4  203.861908        5156.299805                       2125.800049              11.3        23         1\n",
      "24  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   98.000000      13.825000        10.8  226.970474        5176.100098                       2149.300049              10.9        24         1\n",
      "25  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   97.699997      13.624000        10.8  233.030457        5200.200195                       2161.600098              10.9        25         1\n",
      "26  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   97.900002      13.247500        10.4  241.478088        5217.200195                       2174.000000              11.1        26         1\n",
      "27  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   98.000000      13.042500        10.4  256.058411        5218.799805                       2177.000000              11.1        27         1\n",
      "28  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   98.099998      12.800000        10.3  266.452179        5243.299805                       2202.800049              10.6        28         1\n",
      "29  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   98.800003      12.782000        10.2  279.528015        5254.399902                       2226.399902              10.3        29         1\n",
      "30  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   99.199997      12.632500        10.1  302.438110        5264.799805                       2245.899902               9.9        30         1\n",
      "31  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   99.400002      12.870000        10.1  319.349091        5268.899902                       2276.000000               9.1        31         1\n",
      "32  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0   99.800003      13.422000         9.4  316.093994        5335.299805                       2304.399902               9.6        32         1\n",
      "33  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  100.099998      13.810000         9.5  296.873901        5325.299805                       2320.399902               9.2        33         1\n",
      "34  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  100.400002      13.726000         9.2  300.369995        5364.100098                       2334.899902               9.6        34         1\n",
      "35  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  100.800003      13.535000         8.8  286.722870        5412.500000                       2357.600098               9.7        35         1\n",
      "36  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  101.099998      13.435000         8.5  278.666656        5459.700195                       2366.300049              10.3        36         1\n",
      "37  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  101.400002      13.420000         8.3  278.655701        5509.399902                       2393.600098              10.1        37         1\n",
      "38  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  102.099998      13.367500         8.0  282.004272        5535.100098                       2419.399902              10.0        38         1\n",
      "39  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  102.599998      13.232500         7.8  254.386505        5568.200195                       2403.500000              11.7        39         1\n",
      "40  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  102.900002      13.386000         7.8  251.250000        5605.100098                       2431.600098              11.5        40         1\n",
      "41  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  103.300003      13.652500         7.7  245.207001        5643.799805                       2457.500000              11.5        41         1\n",
      "42  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  103.500000      13.942500         7.4  245.243179        5652.500000                       2474.500000              11.1        42         1\n",
      "43  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  103.699997      14.416000         7.2  238.516190        5690.200195                       2495.600098              11.1        43         1\n",
      "44  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  104.099998      14.667500         7.5  232.403809        5697.399902                       2494.600098              11.6        44         1\n",
      "45  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  104.400002      14.470000         7.5  250.346085        5736.200195                       2512.199951              11.8        45         1\n",
      "46  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  104.699997      14.350000         7.3  252.004730        5777.000000                       2533.800049              11.8        46         1\n",
      "47  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  105.099998      14.130000         7.4  247.153915        5755.399902                       2531.300049              11.7        47         1\n",
      "48  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  105.300003      13.638000         7.2  246.398102        5780.299805                       2571.399902              10.9        48         1\n",
      "49  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  105.500000      13.180000         7.3  241.912003        5815.500000                       2582.600098              11.2        49         1\n",
      "50  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  105.699997      13.075000         7.3  260.798645        5813.600098                       2618.800049              10.3        50         1\n",
      "51  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  106.300003      12.920000         7.2  285.521057        5759.700195                       2640.800049               9.1        51         1\n",
      "52  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  106.800003      13.170000         7.2  280.796661        5733.000000                       2648.500000               8.7        52         1\n",
      "53  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  107.000000      13.195000         7.3  280.894775        5828.100098                       2659.500000               9.9        53         1\n",
      "54  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  107.199997      12.914000         7.2  287.460449        5970.200195                       2696.399902              11.1        54         1\n",
      "55  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  107.500000      12.215000         7.4  290.437500        5844.799805                       2689.399902               9.6        55         1\n",
      "56  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  107.699997      12.032500         7.4  302.042725        5857.200195                       2715.699951               9.1        56         1\n",
      "57  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  107.900002      12.186000         7.1  298.247284        5852.899902                       2752.100098               8.2        57         1\n",
      "58  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  108.099998      12.192500         7.1  287.933167        5877.899902                       2794.699951               7.3        58         1\n",
      "59  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  108.500000      12.135000         7.1  285.358246        5907.000000                       2755.800049               9.1        59         1\n",
      "60  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.000000      11.784000         7.0  304.352509        5915.799805                       2771.100098               9.0        60         1\n",
      "61  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.500000      11.260000         7.0  320.232391        5951.100098                       2811.300049               8.6        61         1\n",
      "62  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.900002      10.886000         6.7  328.520905        5961.600098                       2827.100098               8.6        62         1\n",
      "63  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.699997      10.710000         7.2  348.786316        5992.399902                       2820.199951               9.3        63         1\n",
      "64  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.099998      10.080000         7.2  368.231506        6053.600098                       2823.600098               9.9        64         1\n",
      "65  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  108.699997       9.937500         7.1  382.541351        6076.799805                       2835.199951               9.7        65         1\n",
      "66  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.000000      10.144000         7.2  388.487152        6082.799805                       2857.500000               9.3        66         1\n",
      "67  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.400002      10.682500         7.2  398.595245        6078.299805                       2861.699951               9.4        67         1\n",
      "68  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.500000      10.507500         7.0  385.885468        6110.200195                       2881.199951               9.3        68         1\n",
      "69  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  109.599998      10.200000         6.9  375.617157        6119.299805                       2898.600098               9.0        69         1\n",
      "70  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  110.000000      10.007500         7.0  358.293335        6124.899902                       2971.800049               7.2        70         1\n",
      "71  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  110.199997       9.974000         7.0  355.032623        6121.200195                       2932.899902               8.4        71         1\n",
      "72  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  110.400002       9.695000         6.9  358.078949        6125.200195                       2928.399902               8.8        72         1\n",
      "73  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  110.800003       9.310000         6.6  354.921356        6135.799805                       2997.100098               7.0        73         1\n",
      "74  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  111.400002       9.204000         6.6  384.227142        6159.500000                       2935.500000               9.7        74         1\n",
      "75  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  111.800003       9.082500         6.6  411.712646        6192.100098                       3001.699951               8.5        75         1\n",
      "76  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  112.199997       9.035000         6.6  432.204559        6200.000000                       3013.300049               8.5        76         1\n",
      "77  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  112.699997       9.832500         6.3  422.771423        5967.200195                       3038.800049               4.5        77         1\n",
      "78  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  113.000000      10.596000         6.3  416.634003        6209.100098                       3048.399902               8.2        78         1\n",
      "79  50.050049  52.082081  48.028027  50.220219  50.220219  44659096.0  113.500000      10.537500         6.2  423.695923        6200.799805                       3072.800049               7.7        79         1\n",
      "Checking 'time_idx' column in DataFrame:\n",
      "Column exists: True\n",
      "Data type of 'time_idx': int64\n",
      "Data type of 'Close': float32\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "# Interpolating missing data using forward and backward filling\n",
    "merged_data = merged_data.ffill().bfill()\n",
    "\n",
    "# Convert all float data to float32 to ensure compatibility with MPS\n",
    "for col in merged_data.select_dtypes(include=['float64']):\n",
    "    merged_data[col] = merged_data[col].astype('float32')\n",
    "\n",
    "\n",
    "# Limit the dataset to the first 1000 rows\n",
    "limited_data = merged_data.head(100)\n",
    "\n",
    "# Split the limited data into train and test sets\n",
    "train_data = limited_data[:int(0.8 * len(limited_data))]\n",
    "test_data = limited_data[int(0.8 * len(limited_data)):]\n",
    "\n",
    "# Display the shapes of the train and test data\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "train_data = train_data.copy()\n",
    "\n",
    "# Add time_idx and group_id columns for the training data\n",
    "train_data.loc[:, 'time_idx'] = range(len(train_data))\n",
    "train_data.loc[:, 'group_id'] = 1\n",
    "\n",
    "print(train_data.to_string())\n",
    "print(\"Checking 'time_idx' column in DataFrame:\")\n",
    "print(\"Column exists:\", \"time_idx\" in train_data.columns)\n",
    "print(\"Data type of 'time_idx':\", train_data['time_idx'].dtype)\n",
    "\n",
    "# Define the TimeSeriesDataSet for training\n",
    "# Define the TimeSeriesDataSet for training\n",
    "train_dataset = TimeSeriesDataSet(\n",
    "    data=train_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    group_ids=[\"group_id\"],\n",
    "    target=\"Close\",\n",
    "    max_encoder_length=3,  # Adjust to suit your needs\n",
    "    max_prediction_length=5,  # Adjust to suit your needs\n",
    "\n",
    "    time_varying_known_reals=[\"Open\", \"High\", \"Low\", \"Volume\"],  # Example time-varying known reals\n",
    "    time_varying_unknown_reals=[\"Close\"],  # Target variable in the unknown reals\n",
    "    allow_missing_timesteps=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Data type of 'Close':\", train_data['Close'].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "number of targets should be equivalent to number of loss metrics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the DeepAR model using previous timeseries data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m deepar \u001b[38;5;241m=\u001b[39m \u001b[43mDeepAR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using the previously defined TimeSeriesDataSet\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNormalDistributionLoss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define PyTorch Lightning trainer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     13\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     14\u001b[0m     gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Set according to your system's configuration\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/deepar/__init__.py:191\u001b[0m, in \u001b[0;36mDeepAR.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), MultivariateDistributionLoss):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    189\u001b[0m         dataset\u001b[38;5;241m.\u001b[39mmin_prediction_length \u001b[38;5;241m==\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmax_prediction_length\n\u001b[1;32m    190\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultivariate models require constant prediction lenghts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:1672\u001b[0m, in \u001b[0;36mBaseModelWithCovariates.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   1652\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1653\u001b[0m     static_categoricals\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mstatic_categoricals,\n\u001b[1;32m   1654\u001b[0m     time_varying_categoricals_encoder\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m     categorical_groups\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvariable_groups,\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m-> 1672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:1995\u001b[0m, in \u001b[0;36mAutoRegressiveBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lag \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(lags\u001b[38;5;241m.\u001b[39mget(target, [])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall target lags in dataset must be the same but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1994\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_lags\u001b[39m\u001b[38;5;124m\"\u001b[39m, {name: dataset\u001b[38;5;241m.\u001b[39m_get_lagged_names(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m lags})\n\u001b[0;32m-> 1995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:1238\u001b[0m, in \u001b[0;36mBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1237\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer\n\u001b[0;32m-> 1238\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m net\u001b[38;5;241m.\u001b[39mdataset_parameters \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mmulti_target:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/deepar/__init__.py:137\u001b[0m, in \u001b[0;36mDeepAR.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, n_validation_samples, n_plotting_samples, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m targeti \u001b[38;5;129;01min\u001b[39;00m to_list(target):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         targeti \u001b[38;5;129;01min\u001b[39;00m time_varying_reals_encoder\n\u001b[1;32m    136\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargeti\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be real\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# todo: remove this restriction\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, DistributionLoss)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(target, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, MultiLoss) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(target)\n\u001b[1;32m    139\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of targets should be equivalent to number of loss metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m rnn_class \u001b[38;5;241m=\u001b[39m get_rnn(cell_type)\n\u001b[1;32m    142\u001b[0m cont_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreals)\n",
      "\u001b[0;31mAssertionError\u001b[0m: number of targets should be equivalent to number of loss metrics"
     ]
    }
   ],
   "source": [
    "# Define the DeepAR model using previous timeseries data\n",
    "deepar = DeepAR.from_dataset(\n",
    "    train_dataset,  # Using the previously defined TimeSeriesDataSet\n",
    "    learning_rate=0.001,\n",
    "    hidden_size=64,\n",
    "    rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    loss=\"NormalDistributionLoss\",\n",
    "    output_size=1\n",
    ")\n",
    "\n",
    "# Define PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    gpus=1,  # Set according to your system's configuration\n",
    "    gradient_clip_val=0.1\n",
    ")\n",
    "\n",
    "# Fit the model using the previously defined filtered dataset\n",
    "trainer.fit(deepar, train_dataloader=DataLoader(train_dataset, batch_size=32),)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                      | Params\n",
      "----------------------------------------------------\n",
      "0 | model | TemporalFusionTransformer | 19.6 K\n",
      "----------------------------------------------------\n",
      "19.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.6 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n",
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Adjust batch_size and shuffle as needed\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft_lightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumed[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:191\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Ensure batch is a tuple (x, y) or dictionary with appropriate keys\n",
    "        x = batch[\"x\"] if isinstance(batch, dict) else batch[0]\n",
    "        y = batch[\"y\"] if isinstance(batch, dict) else batch[1]\n",
    "        \n",
    "        # Generate predictions and calculate loss\n",
    "        y_hat = self(x)\n",
    "        loss = self.model.loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # You can customize this optimizer or use the model's defaults\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "        return optimizer\n",
    "\n",
    "# Define the TFT model from the training dataset\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    dataset=train_dataset,\n",
    "    learning_rate=0.01,  # You can adjust the learning rate as needed\n",
    "    hidden_size=16,  # Hidden layer size\n",
    "    attention_head_size=4,  # Number of attention heads\n",
    "    dropout=0.1,  # Dropout rate to prevent overfitting\n",
    "    hidden_continuous_size=8,  # Size of continuous variable hidden layer\n",
    "    output_size=1,  # Number of outputs (e.g., 1 for regression tasks)\n",
    ")\n",
    "\n",
    "# Wrap the TFT model inside a LightningModule\n",
    "tft_lightning_module = TFTLightningModule(tft_model)\n",
    "\n",
    "# Setup a PyTorch Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,  # Specify the number of epochs for training\n",
    "    gradient_clip_val=0.1,  # Gradient clipping to prevent exploding gradients\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Adjust batch_size and shuffle as needed\n",
    "trainer.fit(tft_lightning_module, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Encoder categorical: tensor([[[1.7270e+02, 8.0275e+00, 4.1000e+00, 3.9096e+03, 9.5915e+03,\n",
      "          6.8028e+03, 5.2000e+00]],\n",
      "\n",
      "        [[1.9930e+02, 6.1450e+00, 4.7000e+00, 2.2900e+03, 1.1097e+04,\n",
      "          9.0716e+03, 3.7000e+00]],\n",
      "\n",
      "        [[1.0910e+02, 1.0080e+01, 7.2000e+00, 3.6823e+02, 6.0536e+03,\n",
      "          2.8236e+03, 9.9000e+00]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[2.0180e+02, 6.6820e+00, 4.6000e+00, 2.1374e+03, 1.1198e+04,\n",
      "          9.2596e+03, 3.7000e+00]],\n",
      "\n",
      "        [[1.7460e+02, 7.3820e+00, 3.9000e+00, 2.6578e+03, 9.6476e+03,\n",
      "          6.9522e+03, 4.4000e+00]],\n",
      "\n",
      "        [[1.5090e+02, 8.8275e+00, 5.4000e+00, 7.8424e+02, 7.6691e+03,\n",
      "          4.8508e+03, 7.8000e+00]],\n",
      "\n",
      "        [[2.0290e+02, 6.7625e+00, 4.7000e+00, 2.0862e+03, 1.1196e+04,\n",
      "          9.3438e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.0750e+02, 1.2215e+01, 7.4000e+00, 2.9044e+02, 5.8448e+03,\n",
      "          2.6894e+03, 9.6000e+00]],\n",
      "\n",
      "        [[1.8920e+02, 5.8675e+00, 5.4000e+00, 1.8215e+03, 1.0694e+04,\n",
      "          8.2716e+03, 5.0000e+00]],\n",
      "\n",
      "        [[1.8920e+02, 5.8675e+00, 5.4000e+00, 1.8215e+03, 1.0694e+04,\n",
      "          8.2716e+03, 5.0000e+00]],\n",
      "\n",
      "        [[2.0380e+02, 6.5240e+00, 4.7000e+00, 2.1178e+03, 1.1190e+04,\n",
      "          9.3422e+03, 3.3000e+00]],\n",
      "\n",
      "        [[1.9810e+02, 6.3300e+00, 5.0000e+00, 2.2028e+03, 1.0938e+04,\n",
      "          8.9462e+03, 3.2000e+00]],\n",
      "\n",
      "        [[1.1270e+02, 9.8325e+00, 6.3000e+00, 4.2277e+02, 5.9672e+03,\n",
      "          3.0388e+03, 4.5000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7520e+00, 5.4000e+00, 2.1495e+03, 1.1073e+04,\n",
      "          8.5044e+03, 6.6000e+00]],\n",
      "\n",
      "        [[1.9310e+02, 5.9280e+00, 5.2000e+00, 2.0304e+03, 1.0717e+04,\n",
      "          8.5984e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.9310e+02, 5.9280e+00, 5.2000e+00, 2.0304e+03, 1.0717e+04,\n",
      "          8.5984e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.0790e+02, 1.2186e+01, 7.1000e+00, 2.9825e+02, 5.8529e+03,\n",
      "          2.7521e+03, 8.2000e+00]],\n",
      "\n",
      "        [[1.9240e+02, 5.6275e+00, 5.4000e+00, 2.0657e+03, 1.0677e+04,\n",
      "          8.5591e+03, 2.9000e+00]],\n",
      "\n",
      "        [[2.0070e+02, 6.5075e+00, 4.7000e+00, 2.3387e+03, 1.1190e+04,\n",
      "          9.1916e+03, 3.8000e+00]],\n",
      "\n",
      "        [[1.9610e+02, 5.8200e+00, 4.9000e+00, 2.1578e+03, 1.0857e+04,\n",
      "          8.8501e+03, 2.6000e+00]],\n",
      "\n",
      "        [[1.9930e+02, 6.1450e+00, 4.7000e+00, 2.2900e+03, 1.1097e+04,\n",
      "          9.0716e+03, 3.7000e+00]],\n",
      "\n",
      "        [[1.9370e+02, 5.8550e+00, 5.2000e+00, 1.9575e+03, 1.0752e+04,\n",
      "          8.6784e+03, 2.7000e+00]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.9080e+02, 5.7225e+00, 5.5000e+00, 1.9383e+03, 1.0707e+04,\n",
      "          8.3971e+03, 4.3000e+00]],\n",
      "\n",
      "        [[1.9360e+02, 5.7200e+00, 5.1000e+00, 2.0052e+03, 1.0800e+04,\n",
      "          8.6716e+03, 3.2000e+00]],\n",
      "\n",
      "        [[1.9810e+02, 6.2720e+00, 4.9000e+00, 2.2461e+03, 1.0976e+04,\n",
      "          8.9811e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7520e+00, 5.4000e+00, 2.1495e+03, 1.1073e+04,\n",
      "          8.5044e+03, 6.6000e+00]],\n",
      "\n",
      "        [[9.7700e+01, 1.6270e+01, 9.8000e+00, 1.6738e+02, 5.1647e+03,\n",
      "          2.0801e+03, 1.2600e+01]],\n",
      "\n",
      "        [[1.9170e+02, 5.7300e+00, 5.4000e+00, 2.0629e+03, 1.0677e+04,\n",
      "          8.4445e+03, 3.8000e+00]]])\n",
      "  Encoder continuous: tensor([[[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[2.2387e+02, 2.2891e+02, 2.2185e+02, 2.2268e+02, 2.2268e+02,\n",
      "          4.0930e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.5115e+02, 1.5115e+02, 1.4549e+02, 1.4887e+02, 1.4887e+02,\n",
      "          2.1114e+07]],\n",
      "\n",
      "        [[1.9562e+02, 1.9760e+02, 1.9368e+02, 1.9426e+02, 1.9426e+02,\n",
      "          1.5251e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[2.1193e+02, 2.1283e+02, 2.0840e+02, 2.0931e+02, 2.0931e+02,\n",
      "          8.8639e+06]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[5.0556e+01, 5.4595e+01, 5.0300e+01, 5.4209e+01, 5.4209e+01,\n",
      "          2.2834e+07]],\n",
      "\n",
      "        [[5.2693e+01, 5.2798e+01, 5.1056e+01, 5.1056e+01, 5.1056e+01,\n",
      "          5.1968e+06]],\n",
      "\n",
      "        [[1.9339e+02, 1.9520e+02, 1.9215e+02, 1.9309e+02, 1.9309e+02,\n",
      "          1.0150e+07]],\n",
      "\n",
      "        [[1.9626e+02, 1.9931e+02, 1.9596e+02, 1.9868e+02, 1.9868e+02,\n",
      "          1.5600e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[8.6171e+01, 8.6677e+01, 8.4810e+01, 8.5310e+01, 8.5310e+01,\n",
      "          9.6276e+06]],\n",
      "\n",
      "        [[9.0931e+01, 9.2492e+01, 9.0566e+01, 9.0801e+01, 9.0801e+01,\n",
      "          1.7459e+07]],\n",
      "\n",
      "        [[1.5546e+02, 1.5648e+02, 1.5258e+02, 1.5320e+02, 1.5320e+02,\n",
      "          1.7068e+07]],\n",
      "\n",
      "        [[1.7307e+02, 1.7339e+02, 1.6667e+02, 1.7012e+02, 1.7012e+02,\n",
      "          4.5739e+07]],\n",
      "\n",
      "        [[9.4740e+01, 9.4970e+01, 9.1091e+01, 9.3123e+01, 9.3123e+01,\n",
      "          1.8604e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.0788e+02, 1.0851e+02, 1.0193e+02, 1.0308e+02, 1.0308e+02,\n",
      "          6.5533e+07]],\n",
      "\n",
      "        [[2.1408e+02, 2.1524e+02, 2.1198e+02, 2.1320e+02, 2.1320e+02,\n",
      "          1.4541e+07]],\n",
      "\n",
      "        [[1.4420e+02, 1.4640e+02, 1.4419e+02, 1.4595e+02, 1.4595e+02,\n",
      "          1.1313e+07]],\n",
      "\n",
      "        [[2.3244e+02, 2.3536e+02, 2.3125e+02, 2.3512e+02, 2.3512e+02,\n",
      "          1.8176e+07]],\n",
      "\n",
      "        [[9.6732e+01, 9.7277e+01, 9.5145e+01, 9.5821e+01, 9.5821e+01,\n",
      "          1.2293e+07]],\n",
      "\n",
      "        [[1.5263e+02, 1.5413e+02, 1.5152e+02, 1.5179e+02, 1.5179e+02,\n",
      "          1.4141e+07]],\n",
      "\n",
      "        [[7.4089e+01, 7.4570e+01, 6.9870e+01, 7.0315e+01, 7.0315e+01,\n",
      "          2.2723e+07]],\n",
      "\n",
      "        [[1.2649e+02, 1.3062e+02, 1.2544e+02, 1.3054e+02, 1.3054e+02,\n",
      "          3.6080e+07]],\n",
      "\n",
      "        [[2.0771e+02, 2.0810e+02, 2.0448e+02, 2.0480e+02, 2.0480e+02,\n",
      "          1.5272e+07]],\n",
      "\n",
      "        [[8.5260e+01, 8.6927e+01, 8.4449e+01, 8.5075e+01, 8.5075e+01,\n",
      "          1.5069e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[9.0315e+01, 9.4254e+01, 8.9464e+01, 9.2528e+01, 9.2528e+01,\n",
      "          2.3779e+07]]])\n",
      "  Target: tensor([[ 50.2202],\n",
      "        [222.6777],\n",
      "        [ 50.2202],\n",
      "        [148.8689],\n",
      "        [194.2643],\n",
      "        [ 50.2202],\n",
      "        [ 50.2202],\n",
      "        [209.3093],\n",
      "        [ 50.2202],\n",
      "        [ 54.2092],\n",
      "        [ 51.0561],\n",
      "        [193.0931],\n",
      "        [198.6837],\n",
      "        [ 50.2202],\n",
      "        [ 85.3103],\n",
      "        [ 90.8008],\n",
      "        [153.2032],\n",
      "        [170.1201],\n",
      "        [ 93.1231],\n",
      "        [ 50.2202],\n",
      "        [103.0831],\n",
      "        [213.1982],\n",
      "        [145.9510],\n",
      "        [235.1151],\n",
      "        [ 95.8208],\n",
      "        [151.7918],\n",
      "        [ 70.3153],\n",
      "        [130.5355],\n",
      "        [204.8048],\n",
      "        [ 85.0751],\n",
      "        [ 50.2202],\n",
      "        [ 92.5275]])\n",
      "Batch 1:\n",
      "  Encoder categorical: tensor([[[2.0130e+02, 6.5975e+00, 4.6000e+00, 2.2453e+03, 1.1183e+04,\n",
      "          9.2318e+03, 3.6000e+00]],\n",
      "\n",
      "        [[1.9310e+02, 5.9280e+00, 5.2000e+00, 2.0304e+03, 1.0717e+04,\n",
      "          8.5984e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9370e+02, 5.8550e+00, 5.2000e+00, 1.9575e+03, 1.0752e+04,\n",
      "          8.6784e+03, 2.7000e+00]],\n",
      "\n",
      "        [[2.0130e+02, 6.5975e+00, 4.6000e+00, 2.2453e+03, 1.1183e+04,\n",
      "          9.2318e+03, 3.6000e+00]],\n",
      "\n",
      "        [[1.0900e+02, 1.1784e+01, 7.0000e+00, 3.0435e+02, 5.9158e+03,\n",
      "          2.7711e+03, 9.0000e+00]],\n",
      "\n",
      "        [[1.9610e+02, 5.8200e+00, 4.9000e+00, 2.1578e+03, 1.0857e+04,\n",
      "          8.8501e+03, 2.6000e+00]],\n",
      "\n",
      "        [[1.9370e+02, 5.5820e+00, 5.0000e+00, 2.0740e+03, 1.0824e+04,\n",
      "          8.7534e+03, 2.6000e+00]],\n",
      "\n",
      "        [[2.0290e+02, 6.7625e+00, 4.7000e+00, 2.0862e+03, 1.1196e+04,\n",
      "          9.3438e+03, 3.1000e+00]],\n",
      "\n",
      "        [[2.0290e+02, 6.7625e+00, 4.7000e+00, 2.0862e+03, 1.1196e+04,\n",
      "          9.3438e+03, 3.1000e+00]],\n",
      "\n",
      "        [[2.0070e+02, 6.5075e+00, 4.7000e+00, 2.3387e+03, 1.1190e+04,\n",
      "          9.1916e+03, 3.8000e+00]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.9610e+02, 5.8200e+00, 4.9000e+00, 2.1578e+03, 1.0857e+04,\n",
      "          8.8501e+03, 2.6000e+00]],\n",
      "\n",
      "        [[2.0290e+02, 6.7625e+00, 4.7000e+00, 2.0862e+03, 1.1196e+04,\n",
      "          9.3438e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9970e+02, 6.3240e+00, 4.7000e+00, 2.3003e+03, 1.1196e+04,\n",
      "          9.1329e+03, 4.1000e+00]],\n",
      "\n",
      "        [[2.0380e+02, 6.5240e+00, 4.7000e+00, 2.1178e+03, 1.1190e+04,\n",
      "          9.3422e+03, 3.3000e+00]],\n",
      "\n",
      "        [[1.9080e+02, 5.7225e+00, 5.5000e+00, 1.9383e+03, 1.0707e+04,\n",
      "          8.3971e+03, 4.3000e+00]],\n",
      "\n",
      "        [[1.7560e+02, 7.0325e+00, 4.2000e+00, 2.6569e+03, 9.6974e+03,\n",
      "          6.9874e+03, 4.9000e+00]],\n",
      "\n",
      "        [[1.4930e+02, 8.6400e+00, 5.9000e+00, 7.6394e+02, 7.5232e+03,\n",
      "          4.7750e+03, 6.8000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7300e+00, 5.4000e+00, 2.0629e+03, 1.0677e+04,\n",
      "          8.4445e+03, 3.8000e+00]],\n",
      "\n",
      "        [[1.9160e+02, 5.7100e+00, 5.3000e+00, 2.0719e+03, 1.0670e+04,\n",
      "          8.4977e+03, 3.2000e+00]],\n",
      "\n",
      "        [[1.9490e+02, 5.6950e+00, 5.0000e+00, 2.1451e+03, 1.0839e+04,\n",
      "          8.8538e+03, 2.1000e+00]],\n",
      "\n",
      "        [[1.4450e+02, 7.2060e+00, 6.9000e+00, 7.0340e+02, 7.2812e+03,\n",
      "          4.4689e+03, 7.6000e+00]],\n",
      "\n",
      "        [[1.9810e+02, 6.2720e+00, 4.9000e+00, 2.2461e+03, 1.0976e+04,\n",
      "          8.9811e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7520e+00, 5.4000e+00, 2.1495e+03, 1.1073e+04,\n",
      "          8.5044e+03, 6.6000e+00]],\n",
      "\n",
      "        [[1.0110e+02, 1.3435e+01, 8.5000e+00, 2.7867e+02, 5.4597e+03,\n",
      "          2.3663e+03, 1.0300e+01]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.3780e+02, 8.7120e+00, 7.0000e+00, 5.3658e+02, 6.9322e+03,\n",
      "          4.0036e+03, 9.0000e+00]],\n",
      "\n",
      "        [[9.9200e+01, 1.2632e+01, 1.0100e+01, 3.0244e+02, 5.2648e+03,\n",
      "          2.2459e+03, 9.9000e+00]],\n",
      "\n",
      "        [[1.9490e+02, 5.6950e+00, 5.0000e+00, 2.1451e+03, 1.0839e+04,\n",
      "          8.8538e+03, 2.1000e+00]],\n",
      "\n",
      "        [[2.0290e+02, 6.7625e+00, 4.7000e+00, 2.0862e+03, 1.1196e+04,\n",
      "          9.3438e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.2310e+02, 1.1050e+01, 5.2000e+00, 4.1713e+02, 6.6723e+03,\n",
      "          3.5430e+03, 8.4000e+00]],\n",
      "\n",
      "        [[1.8920e+02, 5.8675e+00, 5.4000e+00, 1.8215e+03, 1.0694e+04,\n",
      "          8.2716e+03, 5.0000e+00]]])\n",
      "  Encoder continuous: tensor([[[2.0944e+02, 2.0993e+02, 1.9947e+02, 1.9965e+02, 1.9965e+02,\n",
      "          2.0702e+07]],\n",
      "\n",
      "        [[8.8438e+01, 8.9394e+01, 8.7593e+01, 8.7888e+01, 8.7888e+01,\n",
      "          1.4198e+07]],\n",
      "\n",
      "        [[9.4760e+01, 9.6096e+01, 9.4109e+01, 9.5796e+01, 9.5796e+01,\n",
      "          1.6843e+07]],\n",
      "\n",
      "        [[1.8549e+02, 1.9011e+02, 1.8530e+02, 1.8744e+02, 1.8744e+02,\n",
      "          2.1266e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.4384e+02, 1.4490e+02, 1.4308e+02, 1.4378e+02, 1.4378e+02,\n",
      "          9.5744e+06]],\n",
      "\n",
      "        [[1.4134e+02, 1.4702e+02, 1.4106e+02, 1.4562e+02, 1.4562e+02,\n",
      "          4.5007e+07]],\n",
      "\n",
      "        [[2.0252e+02, 2.0571e+02, 2.0206e+02, 2.0415e+02, 2.0415e+02,\n",
      "          1.1612e+07]],\n",
      "\n",
      "        [[1.9429e+02, 1.9615e+02, 1.9169e+02, 1.9294e+02, 1.9294e+02,\n",
      "          1.1053e+07]],\n",
      "\n",
      "        [[2.2467e+02, 2.2559e+02, 2.1830e+02, 2.1877e+02, 2.1877e+02,\n",
      "          4.5057e+07]],\n",
      "\n",
      "        [[1.6014e+02, 1.6080e+02, 1.5553e+02, 1.5566e+02, 1.5566e+02,\n",
      "          1.8270e+07]],\n",
      "\n",
      "        [[1.4695e+02, 1.4797e+02, 1.4539e+02, 1.4577e+02, 1.4577e+02,\n",
      "          8.9546e+06]],\n",
      "\n",
      "        [[2.1324e+02, 2.1416e+02, 2.0815e+02, 2.1044e+02, 2.1044e+02,\n",
      "          1.2072e+07]],\n",
      "\n",
      "        [[1.7135e+02, 1.7305e+02, 1.7027e+02, 1.7112e+02, 1.7112e+02,\n",
      "          1.4855e+07]],\n",
      "\n",
      "        [[1.9160e+02, 1.9244e+02, 1.8973e+02, 1.9069e+02, 1.9069e+02,\n",
      "          1.1475e+07]],\n",
      "\n",
      "        [[9.3263e+01, 9.6416e+01, 9.0090e+01, 9.0991e+01, 9.0991e+01,\n",
      "          4.4570e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[9.2708e+01, 9.4995e+01, 8.8789e+01, 9.1091e+01, 9.1091e+01,\n",
      "          3.3459e+07]],\n",
      "\n",
      "        [[1.0243e+02, 1.0275e+02, 9.8453e+01, 9.8749e+01, 9.8749e+01,\n",
      "          2.2493e+07]],\n",
      "\n",
      "        [[1.4885e+02, 1.4885e+02, 1.4679e+02, 1.4690e+02, 1.4690e+02,\n",
      "          1.1839e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[2.0285e+02, 2.0553e+02, 2.0152e+02, 2.0553e+02, 2.0553e+02,\n",
      "          1.7802e+07]],\n",
      "\n",
      "        [[9.4670e+01, 9.6747e+01, 9.4645e+01, 9.6051e+01, 9.6051e+01,\n",
      "          1.2196e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.5727e+02, 1.5891e+02, 1.5630e+02, 1.5839e+02, 1.5839e+02,\n",
      "          1.8284e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.5718e+02, 1.5906e+02, 1.5576e+02, 1.5713e+02, 1.5713e+02,\n",
      "          3.9539e+07]],\n",
      "\n",
      "        [[2.1097e+02, 2.1161e+02, 2.0803e+02, 2.1094e+02, 2.1094e+02,\n",
      "          9.9612e+06]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]]])\n",
      "  Target: tensor([[199.6496],\n",
      "        [ 87.8879],\n",
      "        [ 95.7958],\n",
      "        [187.4374],\n",
      "        [ 50.2202],\n",
      "        [143.7788],\n",
      "        [145.6156],\n",
      "        [204.1492],\n",
      "        [192.9429],\n",
      "        [218.7688],\n",
      "        [155.6557],\n",
      "        [145.7708],\n",
      "        [210.4354],\n",
      "        [171.1161],\n",
      "        [190.6907],\n",
      "        [ 90.9910],\n",
      "        [ 50.2202],\n",
      "        [ 50.2202],\n",
      "        [ 91.0911],\n",
      "        [ 98.7487],\n",
      "        [146.8969],\n",
      "        [ 50.2202],\n",
      "        [205.5305],\n",
      "        [ 96.0510],\n",
      "        [ 50.2202],\n",
      "        [158.3884],\n",
      "        [ 50.2202],\n",
      "        [ 50.2202],\n",
      "        [157.1271],\n",
      "        [210.9409],\n",
      "        [ 50.2202],\n",
      "        [ 50.2202]])\n",
      "Batch 2:\n",
      "  Encoder categorical: tensor([[[1.9880e+02, 5.7740e+00, 5.0000e+00, 2.1446e+03, 1.0816e+04,\n",
      "          8.9004e+03, 2.7000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7300e+00, 5.4000e+00, 2.0629e+03, 1.0677e+04,\n",
      "          8.4445e+03, 3.8000e+00]],\n",
      "\n",
      "        [[1.9160e+02, 5.7100e+00, 5.3000e+00, 2.0719e+03, 1.0670e+04,\n",
      "          8.4977e+03, 3.2000e+00]],\n",
      "\n",
      "        [[9.4100e+01, 1.6946e+01, 8.5000e+00, 1.9715e+02, 5.0902e+03,\n",
      "          1.9888e+03, 1.2500e+01]],\n",
      "\n",
      "        [[1.9930e+02, 6.1450e+00, 4.7000e+00, 2.2900e+03, 1.1097e+04,\n",
      "          9.0716e+03, 3.7000e+00]],\n",
      "\n",
      "        [[1.9240e+02, 5.6275e+00, 5.4000e+00, 2.0657e+03, 1.0677e+04,\n",
      "          8.5591e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.0940e+02, 1.0682e+01, 7.2000e+00, 3.9860e+02, 6.0783e+03,\n",
      "          2.8617e+03, 9.4000e+00]],\n",
      "\n",
      "        [[1.6810e+02, 7.8520e+00, 4.1000e+00, 2.8153e+03, 9.1172e+03,\n",
      "          6.4114e+03, 4.4000e+00]],\n",
      "\n",
      "        [[1.9310e+02, 5.9280e+00, 5.2000e+00, 2.0304e+03, 1.0717e+04,\n",
      "          8.5984e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9310e+02, 5.9280e+00, 5.2000e+00, 2.0304e+03, 1.0717e+04,\n",
      "          8.5984e+03, 3.1000e+00]],\n",
      "\n",
      "        [[1.9240e+02, 5.6275e+00, 5.4000e+00, 2.0657e+03, 1.0677e+04,\n",
      "          8.5591e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7520e+00, 5.4000e+00, 2.1495e+03, 1.1073e+04,\n",
      "          8.5044e+03, 6.6000e+00]],\n",
      "\n",
      "        [[1.9490e+02, 5.6950e+00, 5.0000e+00, 2.1451e+03, 1.0839e+04,\n",
      "          8.8538e+03, 2.1000e+00]],\n",
      "\n",
      "        [[1.0410e+02, 1.4667e+01, 7.5000e+00, 2.3240e+02, 5.6974e+03,\n",
      "          2.4946e+03, 1.1600e+01]],\n",
      "\n",
      "        [[1.9910e+02, 6.0650e+00, 5.0000e+00, 2.0871e+03, 1.0870e+04,\n",
      "          8.9385e+03, 2.9000e+00]],\n",
      "\n",
      "        [[1.9360e+02, 5.7200e+00, 5.1000e+00, 2.0052e+03, 1.0800e+04,\n",
      "          8.6716e+03, 3.2000e+00]],\n",
      "\n",
      "        [[1.9970e+02, 6.3240e+00, 4.7000e+00, 2.3003e+03, 1.1196e+04,\n",
      "          9.1329e+03, 4.1000e+00]],\n",
      "\n",
      "        [[1.9160e+02, 5.7100e+00, 5.3000e+00, 2.0719e+03, 1.0670e+04,\n",
      "          8.4977e+03, 3.2000e+00]],\n",
      "\n",
      "        [[1.1000e+02, 1.0007e+01, 7.0000e+00, 3.5829e+02, 6.1249e+03,\n",
      "          2.9718e+03, 7.2000e+00]],\n",
      "\n",
      "        [[1.9490e+02, 5.6950e+00, 5.0000e+00, 2.1451e+03, 1.0839e+04,\n",
      "          8.8538e+03, 2.1000e+00]],\n",
      "\n",
      "        [[1.7640e+02, 7.0775e+00, 4.4000e+00, 1.9339e+03, 9.7050e+03,\n",
      "          7.0042e+03, 5.1000e+00]],\n",
      "\n",
      "        [[1.9370e+02, 5.5820e+00, 5.0000e+00, 2.0740e+03, 1.0824e+04,\n",
      "          8.7534e+03, 2.6000e+00]],\n",
      "\n",
      "        [[9.8100e+01, 1.2800e+01, 1.0300e+01, 2.6645e+02, 5.2433e+03,\n",
      "          2.2028e+03, 1.0600e+01]],\n",
      "\n",
      "        [[2.0070e+02, 6.5075e+00, 4.7000e+00, 2.3387e+03, 1.1190e+04,\n",
      "          9.1916e+03, 3.8000e+00]],\n",
      "\n",
      "        [[1.9360e+02, 5.7200e+00, 5.1000e+00, 2.0052e+03, 1.0800e+04,\n",
      "          8.6716e+03, 3.2000e+00]],\n",
      "\n",
      "        [[1.2410e+02, 1.0196e+01, 5.3000e+00, 4.4758e+02, 6.6605e+03,\n",
      "          3.5666e+03, 8.2000e+00]],\n",
      "\n",
      "        [[1.9170e+02, 5.7300e+00, 5.4000e+00, 2.0629e+03, 1.0677e+04,\n",
      "          8.4445e+03, 3.8000e+00]],\n",
      "\n",
      "        [[2.0380e+02, 6.5240e+00, 4.7000e+00, 2.1178e+03, 1.1190e+04,\n",
      "          9.3422e+03, 3.3000e+00]],\n",
      "\n",
      "        [[1.9370e+02, 5.5820e+00, 5.0000e+00, 2.0740e+03, 1.0824e+04,\n",
      "          8.7534e+03, 2.6000e+00]],\n",
      "\n",
      "        [[1.0570e+02, 1.3075e+01, 7.3000e+00, 2.6080e+02, 5.8136e+03,\n",
      "          2.6188e+03, 1.0300e+01]],\n",
      "\n",
      "        [[1.9490e+02, 5.6950e+00, 5.0000e+00, 2.1451e+03, 1.0839e+04,\n",
      "          8.8538e+03, 2.1000e+00]],\n",
      "\n",
      "        [[1.9810e+02, 6.3300e+00, 5.0000e+00, 2.2028e+03, 1.0938e+04,\n",
      "          8.9462e+03, 3.2000e+00]]])\n",
      "  Encoder continuous: tensor([[[1.4340e+02, 1.4514e+02, 1.4336e+02, 1.4437e+02, 1.4437e+02,\n",
      "          6.8621e+06]],\n",
      "\n",
      "        [[8.5551e+01, 8.7808e+01, 8.4785e+01, 8.6361e+01, 8.6361e+01,\n",
      "          2.2361e+07]],\n",
      "\n",
      "        [[9.7788e+01, 9.8794e+01, 9.7122e+01, 9.7763e+01, 9.7763e+01,\n",
      "          1.3685e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[2.2586e+02, 2.2734e+02, 2.1482e+02, 2.1672e+02, 2.1672e+02,\n",
      "          3.7442e+07]],\n",
      "\n",
      "        [[9.6747e+01, 9.7337e+01, 9.4424e+01, 9.7072e+01, 9.7072e+01,\n",
      "          3.1141e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[8.7738e+01, 9.0090e+01, 8.7192e+01, 8.9394e+01, 8.9394e+01,\n",
      "          2.0823e+07]],\n",
      "\n",
      "        [[9.0596e+01, 9.0691e+01, 8.8789e+01, 9.0080e+01, 9.0080e+01,\n",
      "          2.1899e+07]],\n",
      "\n",
      "        [[9.1517e+01, 9.6637e+01, 9.0591e+01, 9.6592e+01, 9.6592e+01,\n",
      "          7.7047e+07]],\n",
      "\n",
      "        [[8.8088e+01, 8.8188e+01, 8.5360e+01, 8.5801e+01, 8.5801e+01,\n",
      "          1.3728e+07]],\n",
      "\n",
      "        [[1.5120e+02, 1.5533e+02, 1.5105e+02, 1.5511e+02, 1.5511e+02,\n",
      "          2.5218e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.8030e+02, 1.8756e+02, 1.7993e+02, 1.8626e+02, 1.8626e+02,\n",
      "          2.8657e+07]],\n",
      "\n",
      "        [[1.1114e+02, 1.1196e+02, 1.1022e+02, 1.1126e+02, 1.1126e+02,\n",
      "          1.9515e+07]],\n",
      "\n",
      "        [[1.9065e+02, 1.9189e+02, 1.8375e+02, 1.8423e+02, 1.8423e+02,\n",
      "          1.7862e+07]],\n",
      "\n",
      "        [[9.7262e+01, 9.8063e+01, 9.5345e+01, 9.7788e+01, 9.7788e+01,\n",
      "          1.6339e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.4880e+02, 1.4895e+02, 1.4584e+02, 1.4591e+02, 1.4591e+02,\n",
      "          1.5985e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.3764e+02, 1.3879e+02, 1.3385e+02, 1.3754e+02, 1.3754e+02,\n",
      "          4.1724e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[2.0417e+02, 2.0512e+02, 2.0095e+02, 2.0232e+02, 2.0232e+02,\n",
      "          1.6259e+07]],\n",
      "\n",
      "        [[1.1104e+02, 1.1419e+02, 1.1077e+02, 1.1321e+02, 1.1321e+02,\n",
      "          3.5525e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[9.0270e+01, 9.1567e+01, 8.8844e+01, 9.0616e+01, 9.0616e+01,\n",
      "          2.1312e+07]],\n",
      "\n",
      "        [[1.8594e+02, 1.9006e+02, 1.8576e+02, 1.8916e+02, 1.8916e+02,\n",
      "          7.8859e+06]],\n",
      "\n",
      "        [[1.3818e+02, 1.4398e+02, 1.3600e+02, 1.4349e+02, 1.4349e+02,\n",
      "          4.2007e+07]],\n",
      "\n",
      "        [[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01,\n",
      "          4.4659e+07]],\n",
      "\n",
      "        [[1.5294e+02, 1.5646e+02, 1.5105e+02, 1.5616e+02, 1.5616e+02,\n",
      "          2.8592e+07]],\n",
      "\n",
      "        [[1.8612e+02, 1.9214e+02, 1.8469e+02, 1.8988e+02, 1.8988e+02,\n",
      "          3.2679e+07]]])\n",
      "  Target: tensor([[144.3694],\n",
      "        [ 86.3614],\n",
      "        [ 97.7628],\n",
      "        [ 50.2202],\n",
      "        [216.7167],\n",
      "        [ 97.0721],\n",
      "        [ 50.2202],\n",
      "        [ 50.2202],\n",
      "        [ 89.3944],\n",
      "        [ 90.0801],\n",
      "        [ 96.5916],\n",
      "        [ 85.8008],\n",
      "        [155.1051],\n",
      "        [ 50.2202],\n",
      "        [186.2563],\n",
      "        [111.2563],\n",
      "        [184.2342],\n",
      "        [ 97.7878],\n",
      "        [ 50.2202],\n",
      "        [145.9059],\n",
      "        [ 50.2202],\n",
      "        [137.5375],\n",
      "        [ 50.2202],\n",
      "        [202.3223],\n",
      "        [113.2082],\n",
      "        [ 50.2202],\n",
      "        [ 90.6156],\n",
      "        [189.1642],\n",
      "        [143.4935],\n",
      "        [ 50.2202],\n",
      "        [156.1562],\n",
      "        [189.8799]])\n",
      "Row 0:\n",
      "{'encoder_cat': tensor([[  85.6000,   14.2050,    7.5000,  200.6856, 4976.5000, 1826.8000,\n",
      "           11.6000]]), 'encoder_cont': tensor([[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01, 4.4659e+07]]), 'decoder_cat': tensor([[  85.6000,   14.2050,    7.5000,  200.6856, 4976.5000, 1826.8000,\n",
      "           11.6000]]), 'decoder_cont': tensor([[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01, 4.4659e+07]]), 'encoder_lengths': tensor([7]), 'decoder_lengths': tensor([7]), 'target': tensor([50.2202])}\n",
      "Row 1:\n",
      "{'encoder_cat': tensor([[  86.4000,   14.7900,    7.2000,  198.3986, 4999.7998, 1851.7000,\n",
      "           11.4000]]), 'encoder_cont': tensor([[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01, 4.4659e+07]]), 'decoder_cat': tensor([[  86.4000,   14.7900,    7.2000,  198.3986, 4999.7998, 1851.7000,\n",
      "           11.4000]]), 'decoder_cont': tensor([[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01, 4.4659e+07]]), 'encoder_lengths': tensor([7]), 'decoder_lengths': tensor([7]), 'target': tensor([50.2202])}\n",
      "Row 2:\n",
      "{'encoder_cat': tensor([[  87.2000,   14.9040,    7.5000,  198.8176, 4980.3999, 1870.0000,\n",
      "           10.9000]]), 'encoder_cont': tensor([[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01, 4.4659e+07]]), 'decoder_cat': tensor([[  87.2000,   14.9040,    7.5000,  198.8176, 4980.3999, 1870.0000,\n",
      "           10.9000]]), 'decoder_cont': tensor([[5.0050e+01, 5.2082e+01, 4.8028e+01, 5.0220e+01, 5.0220e+01, 4.4659e+07]]), 'encoder_lengths': tensor([7]), 'decoder_lengths': tensor([7]), 'target': tensor([50.2202])}\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the data loader\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(\"  Encoder categorical:\", batch[\"encoder_cat\"])\n",
    "    print(\"  Encoder continuous:\", batch[\"encoder_cont\"])\n",
    "    print(\"  Target:\", batch[\"target\"])\n",
    "\n",
    "    # Stop after a few batches\n",
    "    if i >= 2: \n",
    "        break\n",
    "\n",
    "\n",
    "for i in range(3): # Change 3 to check more rows\n",
    "    print(f\"Row {i}:\")\n",
    "    print(train_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                      | Params\n",
      "------------------------------------------------------\n",
      "0 | tft     | TemporalFusionTransformer | 279 K \n",
      "1 | loss_fn | MeanSquaredError          | 0     \n",
      "------------------------------------------------------\n",
      "279 K     Trainable params\n",
      "0         Non-trainable params\n",
      "279 K     Total params\n",
      "1.119     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'encoder_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 104\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    102\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 52\u001b[0m, in \u001b[0;36mStockPriceTFT.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 52\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(output, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 44\u001b[0m, in \u001b[0;36mStockPriceTFT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:402\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    input dimensions: n_samples x time x variables\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     encoder_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    403\u001b[0m     decoder_lengths \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    404\u001b[0m     x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate in time dimension\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder_lengths'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the TFT model\n",
    "class StockPriceTFT(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config  # Directly assign the config\n",
    "        self.save_hyperparameters(ignore=[\"logging_metrics\"])  # Note the exclusion\n",
    "        self.tft = TemporalFusionTransformer(\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            lstm_layers=config[\"lstm_layers\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            output_size=config[\"output_size\"],\n",
    "            attention_head_size=config[\"attention_head_size\"],\n",
    "            max_encoder_length=config[\"max_encoder_length\"],\n",
    "            static_categoricals=config[\"static_categoricals\"],\n",
    "            static_reals=config[\"static_reals\"],\n",
    "            time_varying_categoricals_encoder=config[\"time_varying_categoricals_encoder\"],\n",
    "            time_varying_categoricals_decoder=config[\"time_varying_categoricals_decoder\"],\n",
    "            categorical_groups=config[\"categorical_groups\"],\n",
    "            time_varying_reals_encoder=config[\"time_varying_reals_encoder\"],\n",
    "            time_varying_reals_decoder=config[\"time_varying_reals_decoder\"],\n",
    "            x_reals=config[\"x_reals\"],\n",
    "            x_categoricals=config[\"x_categoricals\"],\n",
    "            hidden_continuous_size=config[\"hidden_continuous_size\"],\n",
    "            hidden_continuous_sizes=config[\"hidden_continuous_sizes\"],\n",
    "            embedding_sizes=config[\"embedding_sizes\"],\n",
    "            embedding_paddings=config[\"embedding_paddings\"],\n",
    "            embedding_labels=config[\"embedding_labels\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            log_interval=config[\"log_interval\"],\n",
    "            log_val_interval=config[\"log_val_interval\"],\n",
    "            log_gradient_flow=config[\"log_gradient_flow\"],\n",
    "            reduce_on_plateau_patience=config[\"reduce_on_plateau_patience\"],\n",
    "            monotone_constaints=config[\"monotone_constaints\"],\n",
    "            share_single_variable_networks=config[\"share_single_variable_networks\"],\n",
    "            causal_attention=config[\"causal_attention\"],\n",
    "            logging_metrics=config[\"logging_metrics\"]\n",
    "        )\n",
    "        self.loss_fn = MeanSquaredError()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tft(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = self.loss_fn(output, batch[\"target\"])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        loss = self.loss_fn(output, batch[\"target\"])\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.config[\"learning_rate\"])\n",
    "\n",
    "def custom_serialize(obj):\n",
    "    # Custom serialization logic\n",
    "    return str(obj)\n",
    "# Configure the TFT model\n",
    "config = {\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 1,\n",
    "    \"attention_head_size\": 4,\n",
    "    \"max_encoder_length\": 30,\n",
    "    \"static_categoricals\": [],\n",
    "    \"static_reals\": [],\n",
    "    \"time_varying_categoricals_encoder\": [],\n",
    "    \"time_varying_categoricals_decoder\": [],\n",
    "    \"categorical_groups\": {},\n",
    "    \"time_varying_reals_encoder\": [\"price\"],\n",
    "    \"time_varying_reals_decoder\": [\"price\"],\n",
    "    \"x_reals\": [\"price\"],\n",
    "    \"x_categoricals\": [],\n",
    "    \"hidden_continuous_size\": 16,\n",
    "    \"hidden_continuous_sizes\": {},\n",
    "    \"embedding_sizes\": {},\n",
    "    \"embedding_paddings\": [],\n",
    "    \"embedding_labels\": {},\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"log_interval\": 10,\n",
    "    \"log_val_interval\": 10,\n",
    "    \"log_gradient_flow\": False,\n",
    "    \"reduce_on_plateau_patience\": 10,\n",
    "    \"monotone_constaints\": {},\n",
    "    \"share_single_variable_networks\": False,\n",
    "    \"causal_attention\": True,\n",
    "    \"logging_metrics\": None\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create the TFT model\n",
    "model = StockPriceTFT(config)\n",
    "# Train the model\n",
    "\n",
    "trainer = Trainer(max_epochs=100)\n",
    "\n",
    "trainer.fit(model, train_loader, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
