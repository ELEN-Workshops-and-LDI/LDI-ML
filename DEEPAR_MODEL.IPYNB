{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from torchmetrics import MeanSquaredError\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchmetrics import MeanSquaredError\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl # Instead of import pytorch_lightning as pl\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from pytorch_forecasting import DeepAR, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'train_data' and 'test_data' are already defined and preprocessed as per your previous code\n",
    "\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/x_bnq71n58lbk4_znq_twz6h0000gn/T/ipykernel_12274/4176988995.py:40: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  merged_data['time_idx'] = merged_data['Date'].dt.to_period(\"D\").dt.to_timestamp().view(int) / 10**9\n",
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/data/timeseries.py:1281: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 5 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__group_id': 0}, {'__group_id__group_id': 1}, {'__group_id__group_id': 2}, {'__group_id__group_id': 3}, {'__group_id__group_id': 4}]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "filters should not remove entries all entries - check encoder/decoder lengths and lags",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mCategorical(merged_data\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mcodes\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Create the TimeSeriesDataSet\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerged_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# minimum length of history\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# maximum length of history\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_known_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCPI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMortgage_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnemp_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNASDAQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisposable_income\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPersonal_consumption_expenditure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpersonal_savings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_unknown_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHigh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdj Close\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVolume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupNormalizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftplus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# normalize by group\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Prepare DataLoader\u001b[39;00m\n\u001b[1;32m     69\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/data/timeseries.py:481\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget normalizer is separate and not in scalers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# create index\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# convert to torch tensor for high performance data loading later\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_to_tensors(data)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/data/timeseries.py:1289\u001b[0m, in \u001b[0;36mTimeSeriesDataSet._construct_index\u001b[0;34m(self, data, predict_mode)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         missing_groups[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_values(name, missing_groups[\u001b[38;5;28mid\u001b[39m], inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, group_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1281\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin encoder length and/or min_prediction_idx and/or min prediction length and/or lags are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoo large for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28mlen\u001b[39m(df_index) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1291\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters should not remove entries all entries - check encoder/decoder lengths and lags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_index\n",
      "\u001b[0;31mAssertionError\u001b[0m: filters should not remove entries all entries - check encoder/decoder lengths and lags"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "\n",
    "\n",
    "# Sample data provided\n",
    "data_dict = {\n",
    "    \"Date\": pd.to_datetime([\"2004-08-19\", \"2004-08-20\", \"2004-08-23\", \"2004-08-24\", \"2004-08-25\"]),\n",
    "    \"Open\": [50.050049, 50.555557, 55.430431, 55.675674, 52.532532],\n",
    "    \"High\": [52.082081, 54.594597, 56.796799, 55.855858, 54.054054],\n",
    "    \"Low\": [48.028027, 50.300301, 54.579578, 51.836838, 51.991993],\n",
    "    \"Close\": [50.220219, 54.209209, 54.754753, 52.487488, 53.053055],\n",
    "    \"Adj Close\": [50.220219, 54.209209, 54.754753, 52.487488, 53.053055],\n",
    "    \"Volume\": [44659096, 22834343, 18256126, 15247337, 9188602]\n",
    "}\n",
    "\n",
    "macro_data_dict = {\n",
    "    \"date\": pd.to_datetime([\"2004-08-19\", \"2004-08-20\", \"2004-08-23\", \"2004-08-24\", \"2004-08-25\"]),\n",
    "    \"CPI\": [85.6, 86.4, 87.2, 88.0, 88.6],\n",
    "    \"Mortgage_rate\": [14.2050, 14.7900, 14.9040, 15.1325, 15.4000],\n",
    "    \"Unemp_rate\": [7.5, 7.2, 7.5, 7.4, 7.4],\n",
    "    \"NASDAQ\": [200.685556, 198.398636, 198.817619, 194.852105, 203.593182],\n",
    "    \"disposable_income\": [4976.5, 4999.8, 4980.4, 4965.0, 4979.0],\n",
    "    \"Personal_consumption_expenditure\": [1826.8, 1851.7, 1870.0, 1884.2, 1902.9],\n",
    "    \"personal_savings\": [11.6, 11.4, 10.9, 10.8, 10.8]\n",
    "}\n",
    "\n",
    "# Convert dictionaries to DataFrames\n",
    "data = pd.DataFrame(data_dict)\n",
    "macro_data = pd.DataFrame(macro_data_dict)\n",
    "\n",
    "# Merge the dataframes on the date\n",
    "merged_data = pd.merge(data, macro_data, left_on=\"Date\", right_on=\"date\")\n",
    "\n",
    "# Drop the extra date column\n",
    "merged_data.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "# Convert the \"Date\" column to a period index (daily frequency)\n",
    "merged_data['time_idx'] = merged_data['Date'].dt.to_period(\"D\").dt.to_timestamp().view(int) / 10**9\n",
    "merged_data['time_idx'] = merged_data['time_idx'] - merged_data['time_idx'].min()\n",
    "merged_data['time_idx'] = merged_data['time_idx'].astype(int)\n",
    "\n",
    "# Create a simple categorical index for group_ids if not using \"Open\"\n",
    "merged_data['group_id'] = merged_data['Date'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "# Create a simple categorical index for group_ids\n",
    "merged_data['group_id'] = pd.Categorical(merged_data.index).codes\n",
    "\n",
    "# Create the TimeSeriesDataSet\n",
    "dataset = TimeSeriesDataSet(\n",
    "    merged_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Close\",\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=1,  # minimum length of history\n",
    "    max_encoder_length=3,  # maximum length of history\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=1,\n",
    "    static_categoricals=[],\n",
    "    time_varying_known_reals=[\"CPI\", \"Mortgage_rate\", \"Unemp_rate\", \"NASDAQ\", \"disposable_income\", \"Personal_consumption_expenditure\", \"personal_savings\"],\n",
    "    time_varying_unknown_reals=[\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"group_id\"], transformation=\"softplus\"  # normalize by group\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/almazkhalilov/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Encoder and decoder variables have to be the same apart from target variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepAR\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create the DeepAR model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m deepar_model \u001b[38;5;241m=\u001b[39m \u001b[43mDeepAR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNormalDistributionLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/deepar/__init__.py:191\u001b[0m, in \u001b[0;36mDeepAR.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), MultivariateDistributionLoss):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    189\u001b[0m         dataset\u001b[38;5;241m.\u001b[39mmin_prediction_length \u001b[38;5;241m==\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmax_prediction_length\n\u001b[1;32m    190\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultivariate models require constant prediction lenghts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:1672\u001b[0m, in \u001b[0;36mBaseModelWithCovariates.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   1652\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1653\u001b[0m     static_categoricals\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mstatic_categoricals,\n\u001b[1;32m   1654\u001b[0m     time_varying_categoricals_encoder\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m     categorical_groups\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvariable_groups,\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m-> 1672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:1995\u001b[0m, in \u001b[0;36mAutoRegressiveBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lag \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(lags\u001b[38;5;241m.\u001b[39mget(target, [])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall target lags in dataset must be the same but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1994\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_lags\u001b[39m\u001b[38;5;124m\"\u001b[39m, {name: dataset\u001b[38;5;241m.\u001b[39m_get_lagged_names(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m lags})\n\u001b[0;32m-> 1995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/base_model.py:1238\u001b[0m, in \u001b[0;36mBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1237\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer\n\u001b[0;32m-> 1238\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m net\u001b[38;5;241m.\u001b[39mdataset_parameters \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mmulti_target:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pytorch_forecasting/models/deepar/__init__.py:130\u001b[0m, in \u001b[0;36mDeepAR.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, n_validation_samples, n_plotting_samples, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m MultiEmbedding(\n\u001b[1;32m    123\u001b[0m     embedding_sizes\u001b[38;5;241m=\u001b[39membedding_sizes,\n\u001b[1;32m    124\u001b[0m     embedding_paddings\u001b[38;5;241m=\u001b[39membedding_paddings,\n\u001b[1;32m    125\u001b[0m     categorical_groups\u001b[38;5;241m=\u001b[39mcategorical_groups,\n\u001b[1;32m    126\u001b[0m     x_categoricals\u001b[38;5;241m=\u001b[39mx_categoricals,\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m lagged_target_names \u001b[38;5;241m=\u001b[39m [l \u001b[38;5;28;01mfor\u001b[39;00m lags \u001b[38;5;129;01min\u001b[39;00m target_lags\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lags]\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_variables) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(to_list(target)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(lagged_target_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_variables\n\u001b[1;32m    132\u001b[0m ) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(lagged_target_names), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder and decoder variables have to be the same apart from target variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m targeti \u001b[38;5;129;01min\u001b[39;00m to_list(target):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         targeti \u001b[38;5;129;01min\u001b[39;00m time_varying_reals_encoder\n\u001b[1;32m    136\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargeti\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be real\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# todo: remove this restriction\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Encoder and decoder variables have to be the same apart from target variable"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import DeepAR\n",
    "\n",
    "# Create the DeepAR model\n",
    "deepar_model = DeepAR.from_dataset(\n",
    "    dataset,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=64,\n",
    "    rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    loss='NormalDistributionLoss'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=50, gpus=0)\n",
    "trainer.fit(deepar_model, train_dataloader=dataloader)\n",
    "\n",
    "# Save the model\n",
    "torch.save(deepar_model.state_dict(), './models/deepar_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
